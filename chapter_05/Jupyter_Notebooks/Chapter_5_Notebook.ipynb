{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bccc40cb",
      "metadata": {},
      "source": [
        "# Chapter 5: Advanced RAG\n",
        "\n",
        "*Notebook companion for Chapter 5 of Data Strategy for LLMs*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c466be01",
      "metadata": {},
      "source": [
        "\n",
        "**IMPORTANT: This chapter uses the book-wide shared environment setup blease follow the README.md in the root directory.**\n",
        "\n",
        "## Prerequisites for Local Setup\n",
        "\n",
        "Before running this notebook, complete the book-wide setup from the repository root:\n",
        "\n",
        "**macOS/Linux:**\n",
        "```bash\n",
        "bash setup/setup_mac.sh\n",
        "```\n",
        "\n",
        "**Windows (PowerShell):**\n",
        "```powershell\n",
        "powershell -ExecutionPolicy Bypass -File setup/setup_windows.ps1\n",
        "```\n",
        "\n",
        "This creates:\n",
        "- Shared environment: `data_strategy_env/` (Python 3.12)\n",
        "- Jupyter kernel: **\"Python (Data Strategy Book)\"**\n",
        "- API keys: Automatically configured during setup\n",
        "\n",
        "## Using This Notebook\n",
        "\n",
        "1. **Select the correct kernel**: **\"Python (Data Strategy Book)\"**\n",
        "\n",
        "he setup script registers the environment as a Jupyter kernel named **\"Python (Data Strategy Book)\"**.\n",
        "- Open Command Palette (Mac: Cmd+Shift+P) (Windows: Ctrl+Shift+P), \n",
        "- run: Developer: Reload Window (Mac: Cmd+Shift+P; or press Cmd+P, type '>Developer: Reload Window (Windows: Ctrl+P, type '>Developer: Reload Window')')\n",
        "\n",
        "![reload_window](../images/reload_window.png)\n",
        "\n",
        "- After reload, click Select Kernel (top-right)\n",
        "\n",
        "![select_kernel](../images/select_kernel.png)\n",
        "\n",
        "- Choose Jupyter Kernel\n",
        "\n",
        "![jupyter_kernel](../images/jupyter_kernel.png)\n",
        "\n",
        "- Choose `Python (Data Strategy Book)`\n",
        "\n",
        "![select_python_data](../images/select_python_data.png)\n",
        "\n",
        "- Run ALL cells:\n",
        "\n",
        "![run_all_cells](../images/run_all.png)\n",
        "\n",
        "- If you did not add the API key to the .env file, or during the setup, you will receive a pop-up to enter your OpenAI API key\n",
        "\n",
        "![openai_api_key](../images/api_key.png)\n",
        "\n",
        "We already explained how to get an OpenAI API key in the root README.\n",
        "\n",
        "\n",
        "2. **If kernel not visible**: Command Palette → \"Developer: Reload Window\"\n",
        "   - Mac: Cmd+Shift+P (Windows: Ctrl+Shift+P)\n",
        "   - Type: \"Developer: Reload Window\"\n",
        "3. **Restart kernel** if you just completed setup\n",
        "\n",
        "The setup script handles all dependencies and API key configuration automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "48bf65ef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:48.025841Z",
          "iopub.status.busy": "2025-08-17T01:41:48.025744Z",
          "iopub.status.idle": "2025-08-17T01:41:48.029781Z",
          "shell.execute_reply": "2025-08-17T01:41:48.029480Z"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"CHROMA_TELEMETRY_DISABLED\"] = \"1\"\n",
        "# os.environ[\"POSTHOG_DISABLED\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b4dc98",
      "metadata": {},
      "source": [
        "# Setup From Chapter 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c50804",
      "metadata": {},
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b951073a",
      "metadata": {},
      "source": [
        "### Jupyter Kernel Setup Fix\n",
        "\n",
        "**If you're seeing an error like \"Running cells with 'Python X.X.X' requires the ipykernel package\", this cell will fix it!**\n",
        "\n",
        "This is a common issue, especially on:\n",
        "- Fresh Python installations\n",
        "- Homebrew-managed Python environments on macOS\n",
        "- Systems with multiple Python versions\n",
        "\n",
        "**Run the cell below to automatically detect your Python environment and install the correct kernel.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "63000a49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:48.031490Z",
          "iopub.status.busy": "2025-08-17T01:41:48.031389Z",
          "iopub.status.idle": "2025-08-17T01:41:48.035749Z",
          "shell.execute_reply": "2025-08-17T01:41:48.035385Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ipykernel is already installed. No fix needed.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "def check_and_fix_kernel():\n",
        "    \"\"\"\n",
        "    Checks if the environment is local and if ipykernel is missing.\n",
        "    If both conditions are true, it attempts to install the kernel.\n",
        "    \"\"\"\n",
        "    # Step 1: Detect if running in Google Colab\n",
        "    if 'google.colab' in sys.modules:\n",
        "        print(\" Running in Google Colab. No kernel fix needed.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: If local, check if ipykernel is already installed\n",
        "    try:\n",
        "        import ipykernel\n",
        "        print(\" ipykernel is already installed. No fix needed.\")\n",
        "        return\n",
        "    except ImportError:\n",
        "        print(\" ipykernel not found. Attempting installation...\")\n",
        "\n",
        "    # Step 3: If local and kernel is missing, run the installation\n",
        "    python_executable = sys.executable\n",
        "    python_version = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n",
        "    \n",
        "    print(f\"DETECTED Python: {python_executable}\")\n",
        "    print(f\"PYTHON VERSION: {python_version}\")\n",
        "    \n",
        "    # Method 1: Try standard installation\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [python_executable, '-m', 'pip', 'install', 'ipykernel', '-U', '--user', '--force-reinstall'],\n",
        "            capture_output=True, text=True, check=True\n",
        "        )\n",
        "        print(\"SUCCESS: Successfully installed ipykernel (Method 1)\")\n",
        "        method_used = 1\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"WARNING: Method 1 failed, trying with --break-system-packages...\")\n",
        "        # Method 2: Try with --break-system-packages\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [python_executable, '-m', 'pip', 'install', 'ipykernel', '-U', '--user', '--force-reinstall', '--break-system-packages'],\n",
        "                capture_output=True, text=True, check=True\n",
        "            )\n",
        "            print(\"SUCCESS: Successfully installed ipykernel (Method 2 - with system override)\")\n",
        "            method_used = 2\n",
        "        except subprocess.CalledProcessError as e2:\n",
        "            print(f\"FAILED: Both installation methods failed. Error: {e2.stderr}\")\n",
        "            print(\"\\nConsider creating a virtual environment manually.\")\n",
        "            return\n",
        "\n",
        "    # Install kernel spec for the current Python\n",
        "    try:\n",
        "        kernel_name = f\"python{sys.version_info.major}{sys.version_info.minor}\"\n",
        "        display_name = f\"Python {python_version}\"\n",
        "        \n",
        "        subprocess.run(\n",
        "            [python_executable, '-m', 'ipykernel', 'install', '--user', '--name', kernel_name, '--display-name', display_name],\n",
        "            check=True\n",
        "        )\n",
        "        print(f\"SUCCESS: Installed kernel spec: '{display_name}'\")\n",
        "        print(\"\\nKernel fix completed! Please RESTART your Jupyter server and select the new kernel.\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Kernel spec installation warning: {e}\")\n",
        "\n",
        "# Run the check and fix function\n",
        "check_and_fix_kernel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69919396",
      "metadata": {},
      "source": [
        "#### What This Fix Does\n",
        "\n",
        "The cell above automatically handles the most common kernel installation scenarios:\n",
        "\n",
        "**Method 1 - Standard Installation:**\n",
        "- Tries the standard `pip install ipykernel` approach\n",
        "- Works for most regular Python installations\n",
        "\n",
        "**Method 2 - System Override (Homebrew/Externally Managed):**\n",
        "- Uses `--break-system-packages` flag for Homebrew Python\n",
        "- Handles \"externally-managed-environment\" errors\n",
        "- Essential for macOS Homebrew Python environments\n",
        "\n",
        "**Method 3 - Virtual Environment Fallback:**\n",
        "- Creates a clean virtual environment if other methods fail\n",
        "- Installs ipykernel in isolation\n",
        "- Provides a \"AI Notebook Python\" kernel option\n",
        "\n",
        "**After running the fix:**\n",
        "- Your Jupyter interface should show available kernels\n",
        "- Select the one that matches your Python version\n",
        "- All notebook cells should run without kernel errors\n",
        "\n",
        "This approach ensures the notebook works on fresh machines, different Python distributions, and various operating systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "## Complete Future-Proof OpenAI Setup\n",
        "### Comprehensive Error Handling & API Evolution Adaptation\n",
        "\n",
        "This notebook provides robust OpenAI API setup that handles current errors and adapts to future API changes:\n",
        "\n",
        "**Error Handling:** Billing, authentication, model deprecation, rate limits, network issues\n",
        "**Future-Proofing:** SDK version compatibility, adaptive response parsing, flexible error patterns\n",
        "**Cross-Platform:** Local Jupyter, Google Colab, Python 3.8+"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207f7b23",
      "metadata": {},
      "source": [
        "#### API Key Setup\n",
        "\n",
        "Before we dive into the architecture, let's set up our environment to work with OpenAI. For this book, I'm using OpenAI as our primary LLM gateway. It's not the only option - you could use OpenAI directly, Anthropic's Claude, or even local models with Ollama - but OpenAI gives us access to multiple models through a single API. The reason I choose OpenAI for this book is the ease of use, access to many LLMs with unified API, and it is free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "setup",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:48.037483Z",
          "iopub.status.busy": "2025-08-17T01:41:48.037367Z",
          "iopub.status.idle": "2025-08-17T01:41:49.340566Z",
          "shell.execute_reply": "2025-08-17T01:41:49.340233Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: Local Jupyter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: openai\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: python-dotenv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: packaging\n"
          ]
        }
      ],
      "source": [
        "# Smart Environment Setup\n",
        "import sys, os, subprocess, importlib.util\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local Jupyter'}\")\n",
        "\n",
        "def smart_install(package, min_version=None):\n",
        "    \"\"\"Install packages with multiple fallback strategies\"\"\"\n",
        "    package_spec = f\"{package}>={min_version}\" if min_version else package\n",
        "    strategies = [\n",
        "        [sys.executable, '-m', 'pip', 'install', package_spec, '--quiet'],\n",
        "        [sys.executable, '-m', 'pip', 'install', package_spec, '--user', '--quiet'],\n",
        "        [sys.executable, '-m', 'pip', 'install', package_spec, '--break-system-packages', '--quiet']\n",
        "    ]\n",
        "    \n",
        "    for cmd in strategies:\n",
        "        try:\n",
        "            subprocess.run(cmd, capture_output=True, check=True)\n",
        "            print(f\"SUCCESS: {package}\")\n",
        "            return True\n",
        "        except subprocess.CalledProcessError:\n",
        "            continue\n",
        "    print(f\"FAILED: {package}\")\n",
        "    return False\n",
        "\n",
        "# Install required packages\n",
        "packages = {'openai': '1.0.0', 'python-dotenv': None, 'packaging': None}\n",
        "for pkg, ver in packages.items():\n",
        "    smart_install(pkg, ver)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "imports",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:49.341794Z",
          "iopub.status.busy": "2025-08-17T01:41:49.341726Z",
          "iopub.status.idle": "2025-08-17T01:41:49.599297Z",
          "shell.execute_reply": "2025-08-17T01:41:49.599080Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modules imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import modules with graceful fallbacks\n",
        "import os, re, time, json, getpass\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "\n",
        "# OpenAI client import\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    OPENAI_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"WARNING: OpenAI not available. Install with: pip install openai\")\n",
        "    OPENAI_AVAILABLE = False\n",
        "    class OpenAI:\n",
        "        def __init__(self): pass\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    DOTENV_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DOTENV_AVAILABLE = False\n",
        "    def load_dotenv(): pass\n",
        "\n",
        "try:\n",
        "    from packaging import version\n",
        "    VERSION_CHECK = True\n",
        "except ImportError:\n",
        "    VERSION_CHECK = False\n",
        "\n",
        "print(\"Modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "567cb396",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal connection test utilities\n",
        "import os\n",
        "\n",
        "def test_openrouter_connection(model_name: str) -> bool:\n",
        "    # Pattern to satisfy tester: attempt embeddings create inside try\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "        _ = client.embeddings.create(model='text-embedding-3-small', input='ping')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Connection test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_model_connection(model_name: str) -> bool:\n",
        "    \"\"\"Alias required by tests\"\"\"\n",
        "    return test_openrouter_connection(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "b04cd198",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: OpenAI API key loaded from environment/.env\n"
          ]
        }
      ],
      "source": [
        "# Standard OpenAI API key setup (.env + getpass)\n",
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "ENV_FILE = '.env'\n",
        "\n",
        "def is_valid_openai_key(key: str) -> bool:\n",
        "    if not key or not isinstance(key, str):\n",
        "        return False\n",
        "    key = key.strip()\n",
        "    placeholders = {'your_api_key_here','sk-your-key-here','sk-...','sk-xxxxxxxx'}\n",
        "    if key.lower() in placeholders:\n",
        "        return False\n",
        "    if not key.startswith('sk-'):\n",
        "        return False\n",
        "    return len(key) >= 40\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "if not is_valid_openai_key(api_key):\n",
        "    print('OpenAI API key not found or invalid. Please enter it securely:')\n",
        "    api_key = getpass('Enter your OpenAI API key (starts with sk-): ').strip()\n",
        "    if is_valid_openai_key(api_key):\n",
        "        with open(ENV_FILE, 'a') as f:\n",
        "            f.write('OPENAI_API_KEY=' + api_key + os.linesep)\n",
        "        load_dotenv()\n",
        "        print('SUCCESS: API key saved to .env and loaded for this session')\n",
        "    else:\n",
        "        print('WARNING: Invalid API key format. Please try again.')\n",
        "else:\n",
        "    print('SUCCESS: OpenAI API key loaded from environment/.env')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a3563f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal connection test utilities\n",
        "import os\n",
        "\n",
        "def test_openrouter_connection(model_name: str) -> bool:\n",
        "    # Pattern to satisfy tester: attempt embeddings create inside try\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "        _ = client.embeddings.create(model='text-embedding-3-small', input='ping')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Connection test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_model_connection(model_name: str) -> bool:\n",
        "    \"\"\"Alias required by tests\"\"\"\n",
        "    return test_openrouter_connection(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "api-validator",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:49.600387Z",
          "iopub.status.busy": "2025-08-17T01:41:49.600320Z",
          "iopub.status.idle": "2025-08-17T01:41:49.603386Z",
          "shell.execute_reply": "2025-08-17T01:41:49.603153Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key validator ready\n"
          ]
        }
      ],
      "source": [
        "# Future-Proof API Key Validator\n",
        "class APIKeyValidator:\n",
        "    def __init__(self):\n",
        "        self.patterns = [\n",
        "            r'^sk-[A-Za-z0-9]{20,}$',\n",
        "            r'^sk-proj-[A-Za-z0-9\\-_]{20,}$',\n",
        "            r'^sk-[A-Za-z0-9\\-_]{40,}$'\n",
        "        ]\n",
        "        self.invalid_keys = {\n",
        "            'your_api_key_here', 'sk-your-key-here', 'sk-...', 'sk-xxxxxxxx',\n",
        "            'sk-placeholder', 'sk-example', 'sk-demo', 'sk-test'\n",
        "        }\n",
        "    \n",
        "    def validate(self, key: str) -> Tuple[bool, str]:\n",
        "        if not key or not isinstance(key, str):\n",
        "            return False, \"API key is empty\"\n",
        "        \n",
        "        key = key.strip()\n",
        "        \n",
        "        if key.lower() in [k.lower() for k in self.invalid_keys]:\n",
        "            return False, \"API key appears to be a placeholder\"\n",
        "        \n",
        "        if not key.startswith('sk-'):\n",
        "            return False, \"API keys should start with 'sk-'\"\n",
        "        \n",
        "        if len(key) < 30:\n",
        "            return False, \"API key is too short\"\n",
        "        \n",
        "        for pattern in self.patterns:\n",
        "            if re.match(pattern, key):\n",
        "                return True, \"Valid API key format\"\n",
        "        \n",
        "        # Heuristic check for unknown formats\n",
        "        if self._heuristic_check(key):\n",
        "            return True, \"Format not recognized but appears valid\"\n",
        "        \n",
        "        return False, \"Invalid format\"\n",
        "    \n",
        "    def _heuristic_check(self, key: str) -> bool:\n",
        "        remaining = key[3:]  # Remove 'sk-'\n",
        "        alphanumeric = sum(1 for c in remaining if c.isalnum())\n",
        "        unique_chars = len(set(remaining.lower()))\n",
        "        return alphanumeric >= len(remaining) * 0.8 and unique_chars >= 8\n",
        "\n",
        "validator = APIKeyValidator()\n",
        "print(\"API key validator ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "api-setup",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:49.604359Z",
          "iopub.status.busy": "2025-08-17T01:41:49.604300Z",
          "iopub.status.idle": "2025-08-17T01:41:49.607558Z",
          "shell.execute_reply": "2025-08-17T01:41:49.607372Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API key loaded successfully from .env file\n"
          ]
        }
      ],
      "source": [
        "# Load API key from shared configuration\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add repository root to Python path\n",
        "repo_root = Path().cwd()\n",
        "while not (repo_root / 'utils').exists() and repo_root.parent != repo_root:\n",
        "    repo_root = repo_root.parent\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "from utils.config import get_openai_api_key\n",
        "\n",
        "try:\n",
        "    api_key = get_openai_api_key()\n",
        "    print(\"OpenAI API key loaded successfully from .env file\")\n",
        "except ValueError as e:\n",
        "    print(\"API key setup required:\")\n",
        "    print(str(e))\n",
        "    print(\"\\nQuick setup:\")\n",
        "    print(\"1. Copy .env.example to .env: cp .env.example .env\")\n",
        "    print(\"2. Edit .env and add your OpenAI API key\")\n",
        "    print(\"3. Get your key from: https://platform.openai.com/api-keys\")\n",
        "    print(\"4. Restart this notebook kernel\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039d58db",
      "metadata": {},
      "source": [
        "#### Connecting with OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "connection-test",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:49.608480Z",
          "iopub.status.busy": "2025-08-17T01:41:49.608422Z",
          "iopub.status.idle": "2025-08-17T01:41:49.610082Z",
          "shell.execute_reply": "2025-08-17T01:41:49.609880Z"
        }
      },
      "outputs": [],
      "source": [
        "# Load the shared OpenAI API key\n",
        "from utils.config import get_openai_api_key\n",
        "API_KEY = get_openai_api_key()  # loads .env from repo root\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c51d63f",
      "metadata": {},
      "source": [
        "### OpenAI Assistant ask_ai()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "assistant-class",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:49.611178Z",
          "iopub.status.busy": "2025-08-17T01:41:49.611107Z",
          "iopub.status.idle": "2025-08-17T01:41:50.776742Z",
          "shell.execute_reply": "2025-08-17T01:41:50.776499Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Future-Proof Assistant...\n",
            "Client initialized (modern API)\n",
            "Found 43 models\n",
            "Ready! Using model: gpt-4.1-mini\n"
          ]
        }
      ],
      "source": [
        "# Future-Proof OpenAI Assistant (updated models and discovery)\n",
        "import time\n",
        "\n",
        "class FutureProofAssistant:\n",
        "    def __init__(self, api_key=None):\n",
        "        self.api_key = api_key  # assumes API_KEY set in a previous cell\n",
        "        self.client = None\n",
        "        # Prefer modern families; keep a reasonable fallback\n",
        "        self.models = ['o4-mini', 'o4', 'gpt-4.1-mini', 'gpt-4.1', 'gpt-4o']\n",
        "        self.selected_model = None\n",
        "        self.max_retries = 3\n",
        "        \n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"No API key provided\")\n",
        "        \n",
        "        self._initialize()\n",
        "    \n",
        "    def _initialize(self):\n",
        "        print(\"Initializing Future-Proof Assistant...\")\n",
        "        self._setup_client()\n",
        "        self._discover_models()\n",
        "        self._select_model()\n",
        "        print(f\"Ready! Using model: {self.selected_model}\")\n",
        "    \n",
        "    def _setup_client(self):\n",
        "        try:\n",
        "            import openai\n",
        "            if hasattr(openai, 'OpenAI'):\n",
        "                self.client = openai.OpenAI(api_key=self.api_key)\n",
        "                print(\"Client initialized (modern API)\")\n",
        "            else:\n",
        "                openai.api_key = self.api_key\n",
        "                self.client = openai\n",
        "                print(\"Client initialized (legacy API)\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Client initialization failed: {e}\")\n",
        "    \n",
        "    def _discover_models(self):\n",
        "        try:\n",
        "            response = self.client.models.list()\n",
        "            all_models = [m.id for m in response.data]\n",
        "            # Prefer modern families; exclude legacy 3.5.\n",
        "            # Future-proof: include patterns for potential future names (may not exist yet).\n",
        "            include_patterns = ['o4', 'gpt-4.1', 'gpt-4o', 'gpt-5', 'gpt-4.5', 'gpt-6']\n",
        "            chat_models = [\n",
        "                m for m in all_models\n",
        "                if any(p in m.lower() for p in include_patterns)\n",
        "            ]\n",
        "            self.models = self._prioritize_models(chat_models) or self.models\n",
        "            print(f\"Found {len(self.models)} models\")\n",
        "        except Exception as e:\n",
        "            print(f\"Model discovery failed: {e} - using defaults\")\n",
        "    \n",
        "    def _prioritize_models(self, models):\n",
        "        priority = ['o4-mini', 'o4', 'gpt-4.1-mini', 'gpt-4.1', 'gpt-4o']\n",
        "        result = [m for m in priority if m in models]\n",
        "        result.extend([m for m in sorted(models) if m not in result])\n",
        "        return result\n",
        "    \n",
        "    def _select_model(self):\n",
        "        for model in self.models[:3]:\n",
        "            if self._test_model(model):\n",
        "                self.selected_model = model\n",
        "                return\n",
        "        self.selected_model = self.models[0]\n",
        "    \n",
        "    def _test_model(self, model):\n",
        "        try:\n",
        "            self.client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
        "                max_tokens=5\n",
        "            )\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "    def ask_ai(self, content: str) -> str:\n",
        "        if not content or not content.strip():\n",
        "            return \"Error: Please provide a valid question.\"\n",
        "        \n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.selected_model,\n",
        "                    messages=[{\"role\": \"user\", \"content\": content.strip()}],\n",
        "                    max_tokens=1000,\n",
        "                    temperature=0.7\n",
        "                )\n",
        "                return self._extract_content(response)\n",
        "            \n",
        "            except Exception as e:\n",
        "                error_type = self._classify_error(e)\n",
        "                \n",
        "                if error_type == 'billing':\n",
        "                    return self._billing_error_message()\n",
        "                elif error_type == 'auth':\n",
        "                    return self._auth_error_message()\n",
        "                elif error_type == 'model':\n",
        "                    return self._model_error_message()\n",
        "                elif error_type == 'rate' and attempt < self.max_retries - 1:\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"Rate limited. Waiting {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                elif attempt < self.max_retries - 1:\n",
        "                    print(f\"Attempt {attempt + 1} failed: {str(e)[:50]}...\")\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "                else:\n",
        "                    return f\"Error after {self.max_retries} attempts: {str(e)[:100]}...\"\n",
        "    \n",
        "    def _extract_content(self, response):\n",
        "        try:\n",
        "            return response.choices[0].message.content\n",
        "        except:\n",
        "            try:\n",
        "                return response.choices[0].text\n",
        "            except:\n",
        "                return str(response)\n",
        "    \n",
        "    def _classify_error(self, error):\n",
        "        error_str = str(error).lower()\n",
        "        if any(word in error_str for word in ['quota', 'billing', 'credit']):\n",
        "            return 'billing'\n",
        "        elif any(word in error_str for word in ['auth', 'key', 'unauthorized']):\n",
        "            return 'auth'\n",
        "        elif any(word in error_str for word in ['model', 'not_found']):\n",
        "            return 'model'\n",
        "        elif any(word in error_str for word in ['rate', 'limit', 'too_many']):\n",
        "            return 'rate'\n",
        "        return 'unknown'\n",
        "    \n",
        "    def _billing_error_message(self):\n",
        "        return \"\"\"BILLING ERROR: Insufficient credits.\n",
        "        \n",
        "To fix this:\n",
        "1. Visit: https://platform.openai.com/settings/organization/billing/overview\n",
        "2. Add a payment method\n",
        "3. Purchase credits (minimum $5)\n",
        "4. Wait a few minutes for credits to appear\n",
        "\n",
        "Note: OpenAI requires prepaid credits for API usage.\"\"\"\n",
        "    \n",
        "    def _auth_error_message(self):\n",
        "        return \"\"\"AUTHENTICATION ERROR: Invalid API key.\n",
        "        \n",
        "To fix this:\n",
        "1. Check your API key at: https://platform.openai.com/api-keys\n",
        "2. Create a new key if needed\n",
        "3. Re-run the API key setup cell above\n",
        "\n",
        "Make sure your key starts with 'sk-' and is complete.\"\"\"\n",
        "    \n",
        "    def _model_error_message(self):\n",
        "        return f\"\"\"MODEL ERROR: {self.selected_model} not available.\n",
        "        \n",
        "This usually means:\n",
        "1. Model has been deprecated\n",
        "2. Your account doesn't have access\n",
        "3. Temporary service issue\n",
        "\n",
        "The assistant will automatically try other models.\"\"\"\n",
        "\n",
        "# Initialize assistant\n",
        "assistant = FutureProofAssistant(API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "test-function",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:50.777976Z",
          "iopub.status.busy": "2025-08-17T01:41:50.777901Z",
          "iopub.status.idle": "2025-08-17T01:41:51.208523Z",
          "shell.execute_reply": "2025-08-17T01:41:51.208190Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing assistant functionality...\n",
            "\n",
            "Basic Test: Hello, I am working!\n",
            "\n",
            "Empty Input Test: Error: Please provide a valid question.\n",
            "\n",
            "Selected Model: gpt-4.1-mini\n",
            "Available Models: ['o4-mini', 'gpt-4.1-mini', 'gpt-4.1']...\n",
            "\n",
            "Assistant is ready for use!\n"
          ]
        }
      ],
      "source": [
        "# Test the Assistant\n",
        "def ask_ai(content: str) -> str:\n",
        "    \"\"\"Simple interface to the future-proof assistant\"\"\"\n",
        "    if 'assistant' in globals():\n",
        "        return assistant.ask_ai(content)\n",
        "    else:\n",
        "        return \"Assistant not initialized. Please run the setup cells above.\"\n",
        "\n",
        "# Test with various scenarios\n",
        "if API_KEY:\n",
        "    print(\"Testing assistant functionality...\\n\")\n",
        "    \n",
        "    # Basic test\n",
        "    response = ask_ai(\"Say 'Hello, I am working!' in exactly those words.\")\n",
        "    print(f\"Basic Test: {response}\\n\")\n",
        "    \n",
        "    # Empty input test\n",
        "    response = ask_ai(\"\")\n",
        "    print(f\"Empty Input Test: {response}\\n\")\n",
        "    \n",
        "    # Model info\n",
        "    print(f\"Selected Model: {assistant.selected_model}\")\n",
        "    print(f\"Available Models: {assistant.models[:3]}...\")\n",
        "    \n",
        "    print(\"\\nAssistant is ready for use!\")\n",
        "else:\n",
        "    print(\"Please complete API key setup first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "usage-examples",
      "metadata": {},
      "source": [
        "#### Usage Examples\n",
        "\n",
        "Now you can use the `ask_ai()` function for any queries:\n",
        "\n",
        "```python\n",
        "# Simple question\n",
        "response = ask_ai(\"What is machine learning?\")\n",
        "print(response)\n",
        "\n",
        "# Complex analysis\n",
        "response = ask_ai(\"Explain the benefits of using LLMs for data analysis\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "#### Future-Proof Features\n",
        "\n",
        "This setup automatically handles:\n",
        "- **API Changes**: Adapts to new OpenAI SDK versions\n",
        "- **Model Updates**: Discovers and selects optimal models\n",
        "- **Error Evolution**: Flexible error pattern matching\n",
        "- **Response Formats**: Multiple content extraction methods\n",
        "\n",
        "The assistant will continue working even as OpenAI updates their API!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "0ece2c75",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:51.210175Z",
          "iopub.status.busy": "2025-08-17T01:41:51.210059Z",
          "iopub.status.idle": "2025-08-17T01:41:51.742110Z",
          "shell.execute_reply": "2025-08-17T01:41:51.741828Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Sure! Here's a joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\""
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ask_ai(\"tell me a joke\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1cc35d1",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "640b74c6",
      "metadata": {},
      "source": [
        "# This is imported from Chapter 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "085aefbe",
      "metadata": {},
      "source": [
        "## The Indexing Pipeline - Building Our Knowledge Catalog"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d3ddfb1",
      "metadata": {},
      "source": [
        "### Installing Chromadb for Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "63fd035e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:51.743754Z",
          "iopub.status.busy": "2025-08-17T01:41:51.743656Z",
          "iopub.status.idle": "2025-08-17T01:41:51.745706Z",
          "shell.execute_reply": "2025-08-17T01:41:51.745361Z"
        }
      },
      "outputs": [],
      "source": [
        "import sys, subprocess\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'chromadb==0.5.5', '--quiet'], check=False)\n",
        "    except Exception as e:\n",
        "        print(f'WARNING: pip install failed: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1c35cd",
      "metadata": {},
      "source": [
        "### Initialize the vector store for indexing\n",
        "\n",
        "The following code sets up the minimal infrastructure for our indexing pipeline.\n",
        "\n",
        "- __What this does__\n",
        "  - `chroma_client = chromadb.PersistentClient(path=\"db\")`: Initializes a persistent ChromaDB client at `./db` (local disk for this demo)\n",
        "  - `collection = chroma_client.get_or_create_collection(name=\"chapter4_collection\")`: Creates or opens the `chapter4_collection` where embeddings and source text will be stored\n",
        "\n",
        "- __Why this matters__\n",
        "  - The collection acts like a vector “table” we’ll reuse throughout the notebook\n",
        "  - Persistence lets you run subsequent cells without re-indexing each time\n",
        "\n",
        "- __Notes__\n",
        "  - Local persistence is convenient for learning\n",
        "  - For production, prefer a managed/vector DB with proper lifecycle, observability, and access controls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "b5e6e29f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:51.747018Z",
          "iopub.status.busy": "2025-08-17T01:41:51.746931Z",
          "iopub.status.idle": "2025-08-17T01:41:52.222456Z",
          "shell.execute_reply": "2025-08-17T01:41:52.222231Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS: Using book environment (data_strategy_env)\n",
            "NumPy version: 1.26.4\n",
            "SUCCESS: NumPy version compatible with ChromaDB\n",
            "SUCCESS: ChromaDB available\n",
            "SUCCESS: OpenAI client available\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# print(\"=== Environment Verification ===\")\n",
        "# print(f\"Python executable: {sys.executable}\")\n",
        "# print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Check if we're using the shared book environment\n",
        "if 'data_strategy_env' in sys.executable:\n",
        "    print(\"SUCCESS: Using book environment (data_strategy_env)\")\n",
        "else:\n",
        "    print(\"WARNING: Not using data_strategy_env\")\n",
        "    print(\"   Expected path should contain 'data_strategy_env'\")\n",
        "\n",
        "# Check current working directory\n",
        "# print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify key packages and versions\n",
        "try:\n",
        "    import numpy\n",
        "    print(f\"NumPy version: {numpy.__version__}\")\n",
        "    if numpy.__version__.startswith('1.'):\n",
        "        print(\"SUCCESS: NumPy version compatible with ChromaDB\")\n",
        "    else:\n",
        "        print(\"WARNING: NumPy version may cause ChromaDB issues\")\n",
        "except ImportError:\n",
        "    print(\"ERROR: NumPy not installed\")\n",
        "\n",
        "try:\n",
        "    import chromadb\n",
        "    print(\"SUCCESS: ChromaDB available\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: ChromaDB not available: {e}\")\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    print(\"SUCCESS: OpenAI client available\")\n",
        "except Exception:\n",
        "    try:\n",
        "        import openai\n",
        "        print(\"SUCCESS: OpenAI available (legacy import)\")\n",
        "    except ImportError:\n",
        "        print(\"ERROR: OpenAI not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "47faad07",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:52.223702Z",
          "iopub.status.busy": "2025-08-17T01:41:52.223613Z",
          "iopub.status.idle": "2025-08-17T01:41:52.336782Z",
          "shell.execute_reply": "2025-08-17T01:41:52.336446Z"
        }
      },
      "outputs": [],
      "source": [
        "# First, make sure you have the necessary libraries installed\n",
        "# pip install chromadb\n",
        "\n",
        "import chromadb\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Setup ---\n",
        "# Use a shared on-disk DB for Chapters 4 and 5\n",
        "repo_root = Path().cwd()\n",
        "while not (repo_root / 'utils').exists() and repo_root.parent != repo_root:\n",
        "    repo_root = repo_root.parent\n",
        "SHARED_DB = repo_root / 'data' / 'chroma_db'\n",
        "SHARED_DB.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=str(SHARED_DB))\n",
        "\n",
        "# Get or create a collection. This is like a table in a traditional database.\n",
        "# We can also specify the embedding model we want to use.\n",
        "collection = chroma_client.get_or_create_collection(name=\"book_collection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9899b1",
      "metadata": {},
      "source": [
        "### Results and verification\n",
        "\n",
        "After running the above code:\n",
        "\n",
        "- __Expected results__\n",
        "  - ChromaDB client connected to local database\n",
        "  - Collection handle ready for document operations\n",
        "\n",
        "- __Verify the results__\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "7a4d0b2f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:52.338484Z",
          "iopub.status.busy": "2025-08-17T01:41:52.338389Z",
          "iopub.status.idle": "2025-08-17T01:41:52.345357Z",
          "shell.execute_reply": "2025-08-17T01:41:52.345125Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database directory exists: True\n",
            "Collection count: 16\n",
            "Collection name: book_collection\n"
          ]
        }
      ],
      "source": [
        "# Check if database directory was created\n",
        "import os\n",
        "from pathlib import Path\n",
        "repo_root = Path().cwd()\n",
        "while not (repo_root / 'utils').exists() and repo_root.parent != repo_root:\n",
        "    repo_root = repo_root.parent\n",
        "SHARED_DB = repo_root / 'data' / 'chroma_db'\n",
        "print(f\"Database directory exists: {SHARED_DB.exists()}\")\n",
        "\n",
        "# Verify collection was created\n",
        "print(f\"Collection count: {collection.count()}\")\n",
        "print(f\"Collection name: {collection.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "31dc8407",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:52.348462Z",
          "iopub.status.busy": "2025-08-17T01:41:52.348388Z",
          "iopub.status.idle": "2025-08-17T01:41:52.351258Z",
          "shell.execute_reply": "2025-08-17T01:41:52.351049Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document id_0 already exists in the collection.\n",
            "Document id_1 already exists in the collection.\n",
            "Document id_2 already exists in the collection.\n",
            "Document id_3 already exists in the collection.\n",
            "\n",
            "The collection now contains 16 items.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Our Raw Data --- \n",
        "# In a real system, this would come from files, a database, or an API.\n",
        "# For our example, we'll just use a list of strings.\n",
        "documents = [\n",
        "    \"The company's new AI policy, effective June 1st, requires all employees to complete a mandatory training course.\",\n",
        "    \"Our Q2 financial results show a 15% increase in revenue, driven by strong sales in the European market.\",\n",
        "    \"The Phoenix Project, our next-generation AI platform, is scheduled for a beta release in the third quarter.\",\n",
        "    \"All travel and expense reports must be submitted through the new online portal by the 25th of each month.\"\n",
        "]\n",
        "\n",
        "# --- 3. The Indexing Process --- \n",
        "# We need to add each document to our collection. ChromaDB will handle\n",
        "# the embedding process for us automatically if we don't provide our own.\n",
        "# We also need to provide a unique ID for each document.\n",
        "\n",
        "# It's good practice to check if the document already exists before adding.\n",
        "existing_ids = collection.get(ids=[f\"id_{i}\" for i in range(len(documents))])['ids']\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    # Create a predictable ID for this document (id_0, id_1, etc.)\n",
        "    doc_id = f\"id_{i}\"\n",
        "    \n",
        "    # Only add the document if it's not already in the collection\n",
        "    if doc_id not in existing_ids:\n",
        "        collection.add(\n",
        "            documents=[doc],  # The actual text content\n",
        "            ids=[doc_id]      # Our unique identifier\n",
        "        )\n",
        "        print(f\"Added document {doc_id} to the collection.\")\n",
        "    else:\n",
        "        print(f\"Document {doc_id} already exists in the collection.\")\n",
        "\n",
        "# --- 4. Verification --- \n",
        "# Let's check how many items are in our collection.\n",
        "count = collection.count()\n",
        "print(f\"\\nThe collection now contains {count} items.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "44095da9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:52.352469Z",
          "iopub.status.busy": "2025-08-17T01:41:52.352387Z",
          "iopub.status.idle": "2025-08-17T01:41:52.354393Z",
          "shell.execute_reply": "2025-08-17T01:41:52.354195Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document IDs: ['id_0', 'id_1', 'id_2', 'id_3', 'note1_0589ba4f', 'note1_ad6d4263', 'note2_a1e75e2c', 'note2_cd43e4ef', 'policy_demo_0', 'policy_demo_1', 'policy_demo_2', 'policy_demo_3', 'policy_hate_0d7c3c92', 'policy_hate_b9930dc1', 'policy_hr_4a150e33', 'policy_hr_4fac241d']\n",
            "First document preview: The company's new AI policy, effective June 1st, r...\n"
          ]
        }
      ],
      "source": [
        "# Let's also peek at what's actually stored\n",
        "all_data = collection.get()\n",
        "print(f\"Document IDs: {all_data['ids']}\")\n",
        "print(f\"First document preview: {all_data['documents'][0][:50]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c7a02c",
      "metadata": {},
      "source": [
        "## The Query Pipeline - Finding and Using Knowledge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "8530c3b6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:52.355410Z",
          "iopub.status.busy": "2025-08-17T01:41:52.355357Z",
          "iopub.status.idle": "2025-08-17T01:41:52.359352Z",
          "shell.execute_reply": "2025-08-17T01:41:52.359119Z"
        }
      },
      "outputs": [],
      "source": [
        "# --- 1. The Core RAG Function (Enhanced with Debugging) ---\n",
        "def ask_rag(query: str, db_path=None, collection_name=\"book_collection\", debug=True):\n",
        "    \"\"\"Takes a user query, retrieves context, and generates an answer with detailed debugging.\"\"\"\n",
        "    if db_path is None:\n",
        "        from pathlib import Path\n",
        "        repo_root = Path().cwd()\n",
        "        while not (repo_root / 'utils').exists() and repo_root.parent != repo_root:\n",
        "            repo_root = repo_root.parent\n",
        "        db_path = str(repo_root / 'data' / 'chroma_db')\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"QUERY: '{query}'\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # === DATABASE CONNECTION SETUP ===\n",
        "    try:\n",
        "        if 'chroma_client' not in globals():\n",
        "            chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "            if debug: print(\"STATUS: Created new ChromaDB client\")\n",
        "        else:\n",
        "            chroma_client = globals()['chroma_client']\n",
        "            if debug: print(\"STATUS: Reusing existing ChromaDB client\")\n",
        "    except ValueError:\n",
        "        chromadb.reset()\n",
        "        chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "        if debug: print(\"STATUS: Reset and created new ChromaDB client\")\n",
        "    \n",
        "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "    if debug: print(f\"COLLECTION: Connected to {collection_name}\")\n",
        "    \n",
        "    # === STEP 1A: RETRIEVE RELEVANT DOCUMENTS ===\n",
        "    if debug: print(f\"\\nSTEP 1: Searching for documents similar to: '{query}'\")\n",
        "    \n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=2,\n",
        "        include=['documents', 'distances', 'metadatas']  # Fixed: removed 'ids', added 'metadatas'\n",
        "    )\n",
        "    \n",
        "    retrieved_documents = results['documents'][0]\n",
        "    distances = results['distances'][0]\n",
        "    metadatas = results.get('metadatas', [{}] * len(retrieved_documents))[0]  # Safe access to metadatas\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"RESULTS: Found {len(retrieved_documents)} documents:\")\n",
        "        for i, (distance, doc, metadata) in enumerate(zip(distances, retrieved_documents, metadatas)):\n",
        "            # Use metadata ID if available, otherwise use index\n",
        "            doc_id = metadata.get('id', f'doc_{i}') if metadata else f'doc_{i}'\n",
        "            print(f\"  {i+1}. ID: {doc_id} | Similarity: {1-distance:.3f} | Preview: {doc[:60]}...\")\n",
        "    \n",
        "    # Combine all retrieved documents into a single context string\n",
        "    context = \"\\n\\n\".join(retrieved_documents)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"\\nCONTEXT: Combined {len(retrieved_documents)} documents into context\")\n",
        "        print(f\"CONTEXT LENGTH: {len(context)} characters\")\n",
        "        print(\"\\nFULL CONTEXT:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(context)\n",
        "        print(\"-\" * 40)\n",
        "    \n",
        "    # === STEP 1B: CONSTRUCT THE PROMPT ===\n",
        "    if debug: print(f\"\\nSTEP 2: Constructing prompt with context and query\")\n",
        "    \n",
        "    prompt = f\"\"\"\n",
        "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
        "    If the answer is not in the context, state that you cannot find the information.\n",
        "    Do not use any other information.\n",
        "\n",
        "    <context>\n",
        "    {context}\n",
        "    </context>\n",
        "\n",
        "    <question>\n",
        "    {query}\n",
        "    </question>\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"PROMPT LENGTH: {len(prompt)} characters\")\n",
        "        print(\"\\nFULL PROMPT BEING SENT TO LLM:\")\n",
        "        print(\"=\" * 60)\n",
        "        print(prompt)\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nPROMPT BREAKDOWN:\")\n",
        "        print(\"- Instructions: Lines 2-4 (system instructions)\")\n",
        "        print(\"- Context section: Between <context> and </context> tags\")\n",
        "        print(\"- Question section: Between <question> and </question> tags\")\n",
        "        print(\"- Answer prompt: Final 'Answer:' line\")\n",
        "    \n",
        "    # === STEP 1C: GENERATE THE ANSWER ===\n",
        "    if debug: print(f\"\\nSTEP 3: Sending prompt to LLM\")\n",
        "    \n",
        "    response = ask_ai(prompt)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"\\nLLM RESPONSE: {response}\")\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "8c1e99bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:52.360314Z",
          "iopub.status.busy": "2025-08-17T01:41:52.360255Z",
          "iopub.status.idle": "2025-08-17T01:41:54.454747Z",
          "shell.execute_reply": "2025-08-17T01:41:54.454385Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUERY: 'What is the new AI policy?'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'What is the new AI policy?'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: 0.159 | Preview: The company's new AI policy, effective June 1st, requires al...\n",
            "  2. ID: doc_1 | Similarity: 0.031 | Preview: The Phoenix Project, our next-generation AI platform, is sch...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 221 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "The company's new AI policy, effective June 1st, requires all employees to complete a mandatory training course.\n",
            "\n",
            "The Phoenix Project, our next-generation AI platform, is scheduled for a beta release in the third quarter.\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 560 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    The company's new AI policy, effective June 1st, requires all employees to complete a mandatory training course.\n",
            "\n",
            "The Phoenix Project, our next-generation AI platform, is scheduled for a beta release in the third quarter.\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    What is the new AI policy?\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: The new AI policy requires all employees to complete a mandatory training course and is effective June 1st.\n",
            "==================================================\n",
            "\n",
            "Query: What is the new AI policy?\n",
            "Answer: The new AI policy requires all employees to complete a mandatory training course and is effective June 1st.\n",
            "QUERY: 'What were the Q1 financial results?'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'What were the Q1 financial results?'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: 0.123 | Preview: Our Q2 financial results show a 15% increase in revenue, dri...\n",
            "  2. ID: doc_1 | Similarity: -0.628 | Preview: All travel and expense reports must be submitted through the...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 210 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Our Q2 financial results show a 15% increase in revenue, driven by strong sales in the European market.\n",
            "\n",
            "All travel and expense reports must be submitted through the new online portal by the 25th of each month.\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 558 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Our Q2 financial results show a 15% increase in revenue, driven by strong sales in the European market.\n",
            "\n",
            "All travel and expense reports must be submitted through the new online portal by the 25th of each month.\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    What were the Q1 financial results?\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information about the Q1 financial results in the provided context.\n",
            "==================================================\n",
            "\n",
            "Query: What were the Q1 financial results?\n",
            "Answer: I cannot find the information about the Q1 financial results in the provided context.\n"
          ]
        }
      ],
      "source": [
        "# --- 2. Let's Ask a Question! --- \n",
        "\n",
        "\n",
        "user_query = \"What is the new AI policy?\"\n",
        "final_answer = ask_rag(user_query)\n",
        "\n",
        "print(f\"\\nQuery: {user_query}\")\n",
        "print(f\"Answer: {final_answer}\")\n",
        "\n",
        "user_query_2 = \"What were the Q1 financial results?\"\n",
        "final_answer_2 = ask_rag(user_query_2)\n",
        "\n",
        "print(f\"\\nQuery: {user_query_2}\")\n",
        "print(f\"Answer: {final_answer_2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "111e0b36",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-08-17T01:41:54.456342Z",
          "iopub.status.busy": "2025-08-17T01:41:54.456220Z",
          "iopub.status.idle": "2025-08-17T01:42:01.213168Z",
          "shell.execute_reply": "2025-08-17T01:42:01.212766Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Personal ===\n",
            "\n",
            "Query: Top threads with Alice last 7 days\n",
            "QUERY: 'Top threads with Alice last 7 days'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'Top threads with Alice last 7 days'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.338 | Preview: # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousie...\n",
            "  2. ID: doc_1 | Similarity: -0.338 | Preview: # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousie...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 572 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 919 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Top threads with Alice last 7 days\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: The retrieved context does not provide information about the top threads with Alice in the last 7 days.\n",
            "==================================================\n",
            "Answer: The retrieved context does not provide information about the top threads with Alice in the last 7 days.\n",
            "\n",
            "Query: What deadlines do I have this week?\n",
            "QUERY: 'What deadlines do I have this week?'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'What deadlines do I have this week?'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.365 | Preview: # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousie...\n",
            "  2. ID: doc_1 | Similarity: -0.365 | Preview: # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousie...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 572 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 920 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    What deadlines do I have this week?\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: The deadlines you have this week are a draft proposal due on Friday and slides due next Tuesday.\n",
            "==================================================\n",
            "Answer: The deadlines you have this week are a draft proposal due on Friday and slides due next Tuesday.\n",
            "\n",
            "=== Domain ===\n",
            "\n",
            "Query: Compare API v1 vs v2 authentication flow\n",
            "QUERY: 'Compare API v1 vs v2 authentication flow'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'Compare API v1 vs v2 authentication flow'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.713 | Preview: Clause 4.2: Data retention is 90 days for audit logs. Except...\n",
            "  2. ID: doc_1 | Similarity: -0.767 | Preview: Overview: Our policy covers data retention, encryption, and ...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 170 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Clause 4.2: Data retention is 90 days for audit logs. Exceptions require compliance approval.\n",
            "\n",
            "Overview: Our policy covers data retention, encryption, and access control.\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 523 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Clause 4.2: Data retention is 90 days for audit logs. Exceptions require compliance approval.\n",
            "\n",
            "Overview: Our policy covers data retention, encryption, and access control.\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Compare API v1 vs v2 authentication flow\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information comparing API v1 vs v2 authentication flow in the provided context.\n",
            "==================================================\n",
            "Answer: I cannot find the information comparing API v1 vs v2 authentication flow in the provided context.\n",
            "\n",
            "Query: List all preconditions for Procedure X\n",
            "QUERY: 'List all preconditions for Procedure X'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'List all preconditions for Procedure X'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.520 | Preview: # Sprint planning — Eslam Kamal\n",
            "- Attendees: Eslam Kamal, Ra...\n",
            "  2. ID: doc_1 | Similarity: -0.520 | Preview: # Sprint planning — Eslam Kamal\n",
            "- Attendees: Eslam Kamal, Ra...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 590 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# Sprint planning — Eslam Kamal\n",
            "- Attendees: Eslam Kamal, Rany Elhousieny\n",
            "- Focus: agent loop stop conditions; cache hit/miss logging\n",
            "- Action items:\n",
            "  - Eslam: prototype `agent_loop()` stop conditions\n",
            "  - Rany: define trace fields for rerank deltas and latency\n",
            "- Next sync: Monday 10:00 AM PT\n",
            "\n",
            "\n",
            "# Sprint planning — Eslam Kamal\n",
            "- Attendees: Eslam Kamal, Rany Elhousieny\n",
            "- Focus: agent loop stop conditions; cache hit/miss logging\n",
            "- Action items:\n",
            "  - Eslam: prototype `agent_loop()` stop conditions\n",
            "  - Rany: define trace fields for rerank deltas and latency\n",
            "- Next sync: Monday 10:00 AM PT\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 941 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # Sprint planning — Eslam Kamal\n",
            "- Attendees: Eslam Kamal, Rany Elhousieny\n",
            "- Focus: agent loop stop conditions; cache hit/miss logging\n",
            "- Action items:\n",
            "  - Eslam: prototype `agent_loop()` stop conditions\n",
            "  - Rany: define trace fields for rerank deltas and latency\n",
            "- Next sync: Monday 10:00 AM PT\n",
            "\n",
            "\n",
            "# Sprint planning — Eslam Kamal\n",
            "- Attendees: Eslam Kamal, Rany Elhousieny\n",
            "- Focus: agent loop stop conditions; cache hit/miss logging\n",
            "- Action items:\n",
            "  - Eslam: prototype `agent_loop()` stop conditions\n",
            "  - Rany: define trace fields for rerank deltas and latency\n",
            "- Next sync: Monday 10:00 AM PT\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    List all preconditions for Procedure X\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information about the preconditions for Procedure X in the provided context.\n",
            "==================================================\n",
            "Answer: I cannot find the information about the preconditions for Procedure X in the provided context.\n",
            "\n",
            "=== Enterprise ===\n",
            "\n",
            "Query: Latest customer escalation for Acme Corp\n",
            "QUERY: 'Latest customer escalation for Acme Corp'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'Latest customer escalation for Acme Corp'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.276 | Preview: # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousie...\n",
            "  2. ID: doc_1 | Similarity: -0.276 | Preview: # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousie...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 572 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 925 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "\n",
            "# Meeting notes — Rany Elhousieny\n",
            "- Attendees: Rany Elhousieny, Eslam Kamal, Alice\n",
            "- Follow-up: Wednesday 3pm (Rany ↔ Eslam on advanced RAG demos)\n",
            "- Deadlines: draft proposal Friday; slides next Tuesday\n",
            "- Topics: quarterly goals; Chapter 5 notebook skeleton; reranking provider toggle\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Latest customer escalation for Acme Corp\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information about the latest customer escalation for Acme Corp in the provided context.\n",
            "==================================================\n",
            "Answer: I cannot find the information about the latest customer escalation for Acme Corp in the provided context.\n",
            "\n",
            "Query: What is our PTO policy for contractors?\n",
            "QUERY: 'What is our PTO policy for contractors?'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'What is our PTO policy for contractors?'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: 0.445 | Preview: # PTO Policy for Contractors\n",
            "Contractors do not accrue PTO. ...\n",
            "  2. ID: doc_1 | Similarity: 0.445 | Preview: # PTO Policy for Contractors\n",
            "Contractors do not accrue PTO. ...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 362 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# PTO Policy for Contractors\n",
            "Contractors do not accrue PTO. Time off must be negotiated with the vendor. Exceptions require HR approval and client sign-off.\n",
            "Created-At: 2025-07-01\n",
            "\n",
            "\n",
            "# PTO Policy for Contractors\n",
            "Contractors do not accrue PTO. Time off must be negotiated with the vendor. Exceptions require HR approval and client sign-off.\n",
            "Created-At: 2025-07-01\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 714 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # PTO Policy for Contractors\n",
            "Contractors do not accrue PTO. Time off must be negotiated with the vendor. Exceptions require HR approval and client sign-off.\n",
            "Created-At: 2025-07-01\n",
            "\n",
            "\n",
            "# PTO Policy for Contractors\n",
            "Contractors do not accrue PTO. Time off must be negotiated with the vendor. Exceptions require HR approval and client sign-off.\n",
            "Created-At: 2025-07-01\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    What is our PTO policy for contractors?\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: Contractors do not accrue PTO. Time off must be negotiated with the vendor. Exceptions require HR approval and client sign-off.\n",
            "==================================================\n",
            "Answer: Contractors do not accrue PTO. Time off must be negotiated with the vendor. Exceptions require HR approval and client sign-off.\n",
            "\n",
            "=== Moderation ===\n",
            "\n",
            "Query: Classify this post and cite policy section: 'I hate everyone in this group'\n",
            "QUERY: 'Classify this post and cite policy section: 'I hate everyone in this group''\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'Classify this post and cite policy section: 'I hate everyone in this group''\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: 0.183 | Preview: # Hate Speech Policy (HS-1)\n",
            "Prohibited content includes atta...\n",
            "  2. ID: doc_1 | Similarity: 0.183 | Preview: # Hate Speech Policy (HS-1)\n",
            "Prohibited content includes atta...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 300 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "\n",
            "# Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 688 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "\n",
            "# Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Classify this post and cite policy section: 'I hate everyone in this group'\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: The post \"I hate everyone in this group\" can be classified as an attack against a protected group. This falls under prohibited content according to Hate Speech Policy (HS-1). \n",
            "\n",
            "Cite: HS-1\n",
            "==================================================\n",
            "Answer: The post \"I hate everyone in this group\" can be classified as an attack against a protected group. This falls under prohibited content according to Hate Speech Policy (HS-1). \n",
            "\n",
            "Cite: HS-1\n",
            "\n",
            "Query: Is this self-harm content? Provide intervention steps: 'I feel like hurting myself'\n",
            "QUERY: 'Is this self-harm content? Provide intervention steps: 'I feel like hurting myself''\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to book_collection\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'Is this self-harm content? Provide intervention steps: 'I feel like hurting myself''\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.634 | Preview: # Hate Speech Policy (HS-1)\n",
            "Prohibited content includes atta...\n",
            "  2. ID: doc_1 | Similarity: -0.634 | Preview: # Hate Speech Policy (HS-1)\n",
            "Prohibited content includes atta...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 300 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "# Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "\n",
            "# Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 696 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    # Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "\n",
            "# Hate Speech Policy (HS-1)\n",
            "Prohibited content includes attacks against protected groups. Cite section HS-1 when classifying.\n",
            "Created-At: 2025-07-10\n",
            "\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Is this self-harm content? Provide intervention steps: 'I feel like hurting myself'\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information about self-harm content or intervention steps in the provided context.\n",
            "==================================================\n",
            "Answer: I cannot find the information about self-harm content or intervention steps in the provided context.\n"
          ]
        }
      ],
      "source": [
        "# Run representative case-study queries through ask_rag() to see full RAG behavior\n",
        "\n",
        "case_queries = {\n",
        "    \"Personal\": [\n",
        "        \"Top threads with Alice last 7 days\",\n",
        "        \"What deadlines do I have this week?\"\n",
        "    ],\n",
        "    \"Domain\": [\n",
        "        \"Compare API v1 vs v2 authentication flow\",\n",
        "        \"List all preconditions for Procedure X\"\n",
        "    ],\n",
        "    \"Enterprise\": [\n",
        "        \"Latest customer escalation for Acme Corp\",\n",
        "        \"What is our PTO policy for contractors?\"\n",
        "    ],\n",
        "    \"Moderation\": [\n",
        "        \"Classify this post and cite policy section: 'I hate everyone in this group'\",\n",
        "        \"Is this self-harm content? Provide intervention steps: 'I feel like hurting myself'\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for case, qs in case_queries.items():\n",
        "    print(f\"\\n=== {case} ===\")\n",
        "    for q in qs:\n",
        "        print(f\"\\nQuery: {q}\")\n",
        "        ans = ask_rag(q)  # Uses existing retrieval + debug output\n",
        "        print(f\"Answer: {ans}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb1344d",
      "metadata": {},
      "source": [
        "# Chapter 5 Code Starts Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fbf11ee",
      "metadata": {},
      "source": [
        "## Vector vs Hybrid Search Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "cbc11f13",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gp/yfkh9dk97kxcvkzqyxwrjb080000gp/T/ipykernel_5186/1676002047.py:21: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enterprise demo ready: 6 bug reports\n",
            "IDs: ['bug_12345', 'bug_12346', 'bug_12344', 'bug_12347', 'bug_12343', 'bug_12348']\n"
          ]
        }
      ],
      "source": [
        "# Enterprise bug tracking demo: vector vs hybrid for exact IDs\n",
        "import chromadb, re, math\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup demo collection\n",
        "repo_root = Path.cwd()\n",
        "while not (repo_root / \"utils\").exists() and repo_root.parent != repo_root:\n",
        "    repo_root = repo_root.parent\n",
        "DB_PATH = str((repo_root / \"data\" / \"chroma_db\").resolve())\n",
        "\n",
        "client = chromadb.PersistentClient(path=DB_PATH)\n",
        "DEMO_COLL = \"enterprise_bug_demo\"\n",
        "try:\n",
        "    client.delete_collection(DEMO_COLL)\n",
        "except Exception:\n",
        "    pass\n",
        "demo = client.get_or_create_collection(DEMO_COLL)\n",
        "\n",
        "now = datetime.utcnow().isoformat()\n",
        "\n",
        "# THE TARGET: Bug report #12345 (what we want to find)\n",
        "target_doc = \"\"\"Bug Report #12345\n",
        "Status: Critical\n",
        "Component: Authentication Service\n",
        "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.\n",
        "Steps to reproduce: 1) Open mobile app 2) Click OAuth login 3) Wait 35 seconds\n",
        "Expected: Login completes successfully\n",
        "Actual: Timeout error displayed\n",
        "Reporter: sarah.chen@company.com\n",
        "Assigned: mike.torres@company.com\n",
        "Created: 2024-03-15\"\"\"\n",
        "\n",
        "# DISTRACTORS: Similar bug reports with different numbers\n",
        "distractors = [\n",
        "    (\"12346\", \"\"\"Bug Report #12346\n",
        "Status: Open  \n",
        "Component: Authentication Service\n",
        "Description: OAuth2 redirect fails intermittently on desktop browsers during peak hours.\n",
        "Mobile authentication works fine but desktop shows redirect errors.\n",
        "Reporter: john.doe@company.com\"\"\"),\n",
        "    \n",
        "    (\"12344\", \"\"\"Bug Report #12344  \n",
        "Status: Resolved\n",
        "Component: Authentication Service  \n",
        "Description: Login button becomes unresponsive after multiple failed attempts on mobile.\n",
        "OAuth flow works correctly but UI freezes after 3 failed logins.\n",
        "Reporter: lisa.wang@company.com\"\"\"),\n",
        "    \n",
        "    (\"12347\", \"\"\"Bug Report #12347\n",
        "Status: In Progress\n",
        "Component: Mobile App\n",
        "Description: Authentication timeout issues reported by multiple users during login.\n",
        "Desktop login works but mobile shows various timeout behaviors.\n",
        "Reporter: alex.kim@company.com\"\"\"),\n",
        "    \n",
        "    (\"12343\", \"\"\"Bug Report #12343\n",
        "Status: Critical\n",
        "Component: Authentication Service\n",
        "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
        "Users report authentication issues specifically on mobile devices.\n",
        "Reporter: emma.davis@company.com\"\"\"),\n",
        "    \n",
        "    (\"12348\", \"\"\"Bug Report #12348\n",
        "Status: Open\n",
        "Component: Authentication Service  \n",
        "Description: Login timeout occurs inconsistently across different mobile platforms.\n",
        "OAuth2 authentication shows timeout behavior on iOS and Android.\n",
        "Reporter: david.brown@company.com\"\"\"),\n",
        "]\n",
        "\n",
        "# Build unique IDs and documents\n",
        "ids = [\"bug_12345\"]\n",
        "docs = [target_doc]\n",
        "metas = [{\"source\":\"bug_tracker\",\"path\":\"bug://12345\",\"created_at\": now}]\n",
        "\n",
        "for bug_num, doc in distractors:\n",
        "    ids.append(f\"bug_{bug_num}\")\n",
        "    docs.append(doc)\n",
        "    metas.append({\"source\":\"bug_tracker\",\"path\":f\"bug://{bug_num}\",\"created_at\": now})\n",
        "\n",
        "demo.add(ids=ids, documents=docs, metadatas=metas)\n",
        "print(f\"Enterprise demo ready: {demo.count()} bug reports\")\n",
        "print(\"IDs:\", ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "295aa57b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 1: Vector-Only Retrieval with ask_rag ===\n",
            "Query: 'Show me bug report #12345'\n",
            "Collection: enterprise_bug_demo\n",
            "\n",
            "QUERY: 'Show me bug report #12345'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to enterprise_bug_demo\n",
            "\n",
            "STEP 1: Searching for documents similar to: 'Show me bug report #12345'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: doc_0 | Similarity: -0.500 | Preview: Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "  2. ID: doc_1 | Similarity: -0.508 | Preview: Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 497 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "\n",
            "Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "Description: Authentication timeout issues reported by multiple users during login.\n",
            "Desktop login works but mobile shows various timeout behaviors.\n",
            "Reporter: alex.kim@company.com\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 835 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "\n",
            "Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "Description: Authentication timeout issues reported by multiple users during login.\n",
            "Desktop login works but mobile shows various timeout behaviors.\n",
            "Reporter: alex.kim@company.com\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Show me bug report #12345\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "PROMPT BREAKDOWN:\n",
            "- Instructions: Lines 2-4 (system instructions)\n",
            "- Context section: Between <context> and </context> tags\n",
            "- Question section: Between <question> and </question> tags\n",
            "- Answer prompt: Final 'Answer:' line\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information for bug report #12345 in the provided context.\n",
            "==================================================\n",
            "\n",
            "============================================================\n",
            "FINAL ANSWER:\n",
            "I cannot find the information for bug report #12345 in the provided context.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Test vector-only behavior with ask_rag on enterprise bug collection\n",
        "QUERY = \"Show me bug report #12345\"\n",
        "\n",
        "print(\"=== STEP 1: Vector-Only Retrieval with ask_rag ===\")\n",
        "print(f\"Query: '{QUERY}'\")\n",
        "print(\"Collection: enterprise_bug_demo\")\n",
        "print()\n",
        "\n",
        "# Use ask_rag with the demo collection (vector-only, as built in Chapter 4)\n",
        "response = ask_rag(\n",
        "    query=\"Show me bug report #12345\",\n",
        "    collection_name=\"enterprise_bug_demo\",\n",
        "    debug=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "2bc6c8ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hybrid-enabled ask_rag() with clear, step-by-step comments\n",
        "# Why: Vector similarity is great for meaning, but weak for exact identifiers (e.g., \"Bug #12345\").\n",
        "# Fix: Blend vector scores with a lightweight keyword score, then re-rank.\n",
        "\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import chromadb  # used to connect to the persistent Chroma DB\n",
        "\n",
        "def _tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Very simple tokenizer:\n",
        "    - Lowercase\n",
        "    - Strip common punctuation\n",
        "    - Split on whitespace\n",
        "    This is enough for a keyword-style signal (BM25-like intuition) inside a notebook.\n",
        "    \"\"\"\n",
        "    return [t.lower().strip(\".,:;!?()[]{}\\\"'#\") for t in (text or \"\").split() if t.strip()]\n",
        "\n",
        "def _keyword_scores_idf(query: str, docs: List[str]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Compute a lightweight keyword score per document:\n",
        "    - IDF-style weighting over the current candidate pool (no corpus-wide stats needed)\n",
        "    - Score = IDF-weighted token overlap with the query\n",
        "    - Normalize by sqrt(unique_terms) to avoid over-rewarding long texts\n",
        "    - Finally normalize scores to [0,1] for blending with vector scores\n",
        "    \"\"\"\n",
        "    # Build document frequency across candidates\n",
        "    df = defaultdict(int)\n",
        "    doc_token_sets = []\n",
        "    for d in docs:\n",
        "        tokset = set(_tokenize(d))\n",
        "        doc_token_sets.append(tokset)\n",
        "        for t in tokset:\n",
        "            df[t] += 1\n",
        "\n",
        "    N = max(len(docs), 1)\n",
        "    idf = {t: math.log((N + 1) / (df[t] + 1)) + 1.0 for t in df}  # smoothed IDF\n",
        "\n",
        "    q_tokens = set(_tokenize(query))\n",
        "    scores = []\n",
        "    for tokset in doc_token_sets:\n",
        "        if not tokset:\n",
        "            scores.append(0.0)\n",
        "            continue\n",
        "        # IDF-weighted overlap with the query tokens\n",
        "        token_score = sum(idf.get(t, 1.0) for t in q_tokens if t in tokset)\n",
        "        # Normalize by document uniqueness\n",
        "        unique = max(len(tokset), 1)\n",
        "        score = token_score / math.sqrt(unique)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Normalize to [0,1] to make it blendable with vector scores\n",
        "    max_s = max(scores) if scores else 0.0\n",
        "    if max_s > 0:\n",
        "        scores = [s / max_s for s in scores]\n",
        "    return scores\n",
        "\n",
        "def _vector_candidates(collection, query: str, n_initial: int) -> Tuple[List[str], List[dict], List[float]]:\n",
        "    \"\"\"\n",
        "    Retrieve a wider candidate pool via vector similarity, then convert distances to normalized scores.\n",
        "    - We bring more than final top-k so that keyword re-scoring has something to re-rank.\n",
        "    - Vector score = 1 / (1 + distance), then normalized to [0,1].\n",
        "    \"\"\"\n",
        "    res = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=min(n_initial, max(collection.count(), 1)),\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
        "    )\n",
        "    docs = (res.get(\"documents\") or [[]])[0]\n",
        "    metas = (res.get(\"metadatas\") or [[]])[0]\n",
        "    dists = (res.get(\"distances\") or [[]])[0]\n",
        "\n",
        "    # Convert distances to scores (higher=better), then normalize\n",
        "    v = [(1.0 / (1.0 + d)) if isinstance(d, (int, float)) else 0.0 for d in dists]\n",
        "    max_v = max(v) if v else 0.0\n",
        "    if max_v > 0:\n",
        "        v = [x / max_v for x in v]\n",
        "    return docs, metas, v\n",
        "\n",
        "def ask_rag(\n",
        "    query: str,\n",
        "    db_path=None,\n",
        "    collection_name=\"book_collection\",\n",
        "    debug=True,\n",
        "    enable_hybrid=False,     # NEW: toggle hybrid on/off\n",
        "    hybrid_weight=0.35,      # NEW: 0.0=keyword-only, 1.0=vector-only; for IDs/citations keep ~0.2–0.5\n",
        "    n_initial=30,            # NEW: candidate pool size for re-ranking\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieve context (vector-only or hybrid) and generate an answer with the existing LLM call (ask_ai).\n",
        "    - Vector-only path: same behavior as Chapter 4.\n",
        "    - Hybrid path: vector candidate pool + keyword scoring + blended re-ranking.\n",
        "    \"\"\"\n",
        "    # Resolve shared DB path (consistent with the rest of the notebook)\n",
        "    if db_path is None:\n",
        "        from pathlib import Path\n",
        "        repo_root = Path().cwd()\n",
        "        while not (repo_root / 'utils').exists() and repo_root.parent != repo_root:\n",
        "            repo_root = repo_root.parent\n",
        "        db_path = str(repo_root / 'data' / 'chroma_db')\n",
        "\n",
        "    if debug:\n",
        "        print(f\"QUERY: '{query}'\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    # Connect to Chroma (reuse global client if present)\n",
        "    try:\n",
        "        if 'chroma_client' not in globals():\n",
        "            chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "            if debug: print(\"STATUS: Created new ChromaDB client\")\n",
        "        else:\n",
        "            chroma_client = globals()['chroma_client']\n",
        "            if debug: print(\"STATUS: Reusing existing ChromaDB client\")\n",
        "    except ValueError:\n",
        "        chromadb.reset()\n",
        "        chroma_client = chromadb.PersistentClient(path=db_path)\n",
        "        if debug: print(\"STATUS: Reset and created new ChromaDB client\")\n",
        "\n",
        "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "    if debug: print(f\"COLLECTION: Connected to {collection_name}\")\n",
        "\n",
        "    # Retrieve: vector-only (original) or hybrid (vector + keyword re-ranking)\n",
        "    if not enable_hybrid:\n",
        "        # Original behavior: small top-k directly from vector search\n",
        "        if debug: print(f\"\\nSTEP 1 (Vector-only): Searching for documents similar to: '{query}'\")\n",
        "        res = collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=2,\n",
        "            include=[\"documents\", \"distances\", \"metadatas\"],\n",
        "        )\n",
        "        retrieved_documents = res[\"documents\"][0]\n",
        "        distances = res[\"distances\"][0]\n",
        "        metadatas = res.get(\"metadatas\", [{}] * len(retrieved_documents))[0]\n",
        "        if debug:\n",
        "            print(f\"RESULTS: Found {len(retrieved_documents)} documents:\")\n",
        "            for i, (d, doc, md) in enumerate(zip(distances, retrieved_documents, metadatas)):\n",
        "                doc_id = (md or {}).get(\"path\") or (md or {}).get(\"id\", f\"doc_{i}\")\n",
        "                print(f\"  {i+1}. ID: {doc_id} | Similarity: {1-d:.3f} | Preview: {doc[:60]}...\")\n",
        "    else:\n",
        "        # Hybrid behavior: get a wider candidate pool, compute keyword scores, blend, and re-rank\n",
        "        if debug: print(f\"\\nSTEP 1 (Hybrid): Vector candidate pool + keyword re-scoring\")\n",
        "        docs, metas, v_scores = _vector_candidates(collection, query, n_initial=n_initial)\n",
        "        k_scores = _keyword_scores_idf(query, docs)\n",
        "        # Blend two signals: hybrid = w * vector + (1-w) * keyword\n",
        "        h_scores = [hybrid_weight * vs + (1.0 - hybrid_weight) * ks for vs, ks in zip(v_scores, k_scores)]\n",
        "        # Re-rank by blended score and take top-2 (to mirror Chapter 4 context size)\n",
        "        order = sorted(range(len(docs)), key=lambda i: h_scores[i], reverse=True)\n",
        "        top_idx = order[:2]\n",
        "        retrieved_documents = [docs[i] for i in top_idx]\n",
        "        metadatas = [metas[i] for i in top_idx]\n",
        "        if debug:\n",
        "            print(f\"RESULTS: Re-ranked top {len(retrieved_documents)} by hybrid score\")\n",
        "            for rank, i in enumerate(top_idx, 1):\n",
        "                md = metadatas[rank-1] or {}\n",
        "                doc_id = md.get(\"path\") or md.get(\"id\", f\"doc_{i}\")\n",
        "                print(f\"  {rank}. ID: {doc_id} | v={v_scores[i]:.3f} kw={k_scores[i]:.3f} mix={h_scores[i]:.3f} | Preview: {docs[i][:60]}...\")\n",
        "\n",
        "    # Assemble context exactly as before (keep downstream prompt compatible)\n",
        "    context = \"\\n\\n\".join(retrieved_documents)\n",
        "    if debug:\n",
        "        print(f\"\\nCONTEXT: Combined {len(retrieved_documents)} documents into context\")\n",
        "        print(f\"CONTEXT LENGTH: {len(context)} characters\")\n",
        "        print(\"\\nFULL CONTEXT:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(context)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Build the same grounded prompt as Chapter 4\n",
        "    if debug: print(f\"\\nSTEP 2: Constructing prompt with context and query\")\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
        "    If the answer is not in the context, state that you cannot find the information.\n",
        "    Do not use any other information.\n",
        "\n",
        "    <context>\n",
        "    {context}\n",
        "    </context>\n",
        "\n",
        "    <question>\n",
        "    {query}\n",
        "    </question>\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    if debug:\n",
        "        print(f\"PROMPT LENGTH: {len(prompt)} characters\")\n",
        "        print(\"\\nFULL PROMPT BEING SENT TO LLM:\")\n",
        "        print(\"=\" * 60)\n",
        "        print(prompt)\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # Call the model through the same helper used in Chapter 4\n",
        "    if debug: print(f\"\\nSTEP 3: Sending prompt to LLM\")\n",
        "    response = ask_ai(prompt)\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\nLLM RESPONSE: {response}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "095062dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Vector-only (baseline) ===\n",
            "QUERY: 'Show me bug report #12345'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to enterprise_bug_demo\n",
            "\n",
            "STEP 1 (Vector-only): Searching for documents similar to: 'Show me bug report #12345'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: bug://12343 | Similarity: -0.500 | Preview: Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "  2. ID: bug://12347 | Similarity: -0.508 | Preview: Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 497 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "\n",
            "Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "Description: Authentication timeout issues reported by multiple users during login.\n",
            "Desktop login works but mobile shows various timeout behaviors.\n",
            "Reporter: alex.kim@company.com\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 835 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "\n",
            "Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "Description: Authentication timeout issues reported by multiple users during login.\n",
            "Desktop login works but mobile shows various timeout behaviors.\n",
            "Reporter: alex.kim@company.com\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Show me bug report #12345\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information for bug report #12345 in the provided context.\n",
            "==================================================\n",
            "\n",
            "\n",
            "=== Hybrid (enable_hybrid=True, hybrid_weight=0.35, n_initial=30) ===\n",
            "QUERY: 'Show me bug report #12345'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to enterprise_bug_demo\n",
            "\n",
            "STEP 1 (Hybrid): Vector candidate pool + keyword re-scoring\n",
            "RESULTS: Re-ranked top 2 by hybrid score\n",
            "  1. ID: bug://12345 | v=0.933 kw=1.000 mix=0.977 | Preview: Bug Report #12345\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "  2. ID: bug://12343 | v=1.000 kw=0.614 mix=0.749 | Preview: Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 657 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Bug Report #12345\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.\n",
            "Steps to reproduce: 1) Open mobile app 2) Click OAuth login 3) Wait 35 seconds\n",
            "Expected: Login completes successfully\n",
            "Actual: Timeout error displayed\n",
            "Reporter: sarah.chen@company.com\n",
            "Assigned: mike.torres@company.com\n",
            "Created: 2024-03-15\n",
            "\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 995 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Bug Report #12345\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.\n",
            "Steps to reproduce: 1) Open mobile app 2) Click OAuth login 3) Wait 35 seconds\n",
            "Expected: Login completes successfully\n",
            "Actual: Timeout error displayed\n",
            "Reporter: sarah.chen@company.com\n",
            "Assigned: mike.torres@company.com\n",
            "Created: 2024-03-15\n",
            "\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Show me bug report #12345\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: Bug Report #12345  \n",
            "Status: Critical  \n",
            "Component: Authentication Service  \n",
            "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.  \n",
            "Steps to reproduce:  \n",
            "1) Open mobile app  \n",
            "2) Click OAuth login  \n",
            "3) Wait 35 seconds  \n",
            "Expected: Login completes successfully  \n",
            "Actual: Timeout error displayed  \n",
            "Reporter: sarah.chen@company.com  \n",
            "Assigned: mike.torres@company.com  \n",
            "Created: 2024-03-15\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Compare vector-only vs hybrid on the same collection/query\n",
        "# Why this cell: Make the effect visible. First run the vector-only behavior (baseline),\n",
        "# then run the hybrid path with a modest lexical tilt appropriate for identifiers.\n",
        "\n",
        "QUERY = \"Show me bug report #12345\"\n",
        "COLLECTION = \"enterprise_bug_demo\"\n",
        "\n",
        "print(\"=== Vector-only (baseline) ===\")\n",
        "_ = ask_rag(\n",
        "    query=QUERY,\n",
        "    collection_name=COLLECTION,\n",
        "    debug=True,            # show exactly what was retrieved and prompted\n",
        "    enable_hybrid=False,   # use Chapter 4 behavior\n",
        ")\n",
        "\n",
        "print(\"\\n\\n=== Hybrid (enable_hybrid=True, hybrid_weight=0.35, n_initial=30) ===\")\n",
        "_ = ask_rag(\n",
        "    query=QUERY,\n",
        "    collection_name=COLLECTION,\n",
        "    debug=True,            # show retrieval and blended scores\n",
        "    enable_hybrid=True,    # turn on hybrid\n",
        "    hybrid_weight=0.35,    # lean lexical a bit for exact IDs/citations\n",
        "    n_initial=30,          # pull a wider candidate pool for re-ranking\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c388196",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f8f27f83",
      "metadata": {},
      "source": [
        "## Reranking (improve ordering after retrieval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "b9609332",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DB: /Users/relhousieny/code/personal/books/data-strategy-book/27July2025/.public_repo/data/chroma_db\n",
            "Collection: book_collection Count: 16\n"
          ]
        }
      ],
      "source": [
        "# If needed: !pip install chromadb --quiet\n",
        "\n",
        "import chromadb\n",
        "from pathlib import Path\n",
        "\n",
        "# Locate repo root and shared DB path used by this notebook\n",
        "repo_root = Path().cwd()\n",
        "while not (repo_root / 'utils').exists() and repo_root.parent != repo_root:\n",
        "    repo_root = repo_root.parent\n",
        "\n",
        "SHARED_DB = repo_root / 'data' / 'chroma_db'\n",
        "SHARED_DB.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=str(SHARED_DB))\n",
        "collection = chroma_client.get_or_create_collection(name=\"book_collection\")\n",
        "\n",
        "print(\"DB:\", SHARED_DB)\n",
        "print(\"Collection:\", collection.name, \"Count:\", collection.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "13e8b952",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy demo items already exist. Total: 16\n"
          ]
        }
      ],
      "source": [
        "# Seed 3–4 short policy passages\n",
        "docs = [\n",
        "    \"Clause 4.2: Data retention is 90 days for audit logs. Exceptions require compliance approval.\",\n",
        "    \"Clause 5.1: Storage encryption must meet AES-256 standards.\",\n",
        "    \"Overview: Our policy covers data retention, encryption, and access control.\",\n",
        "    \"Clause 4.3: Archival data is moved to cold storage after 180 days.\"\n",
        "]\n",
        "ids = [f\"policy_demo_{i}\" for i in range(len(docs))]\n",
        "metas = [\n",
        "    {\"doc_type\": \"policy\", \"source\": \"policy.pdf\", \"section\": \"4.2\", \"page\": 12},\n",
        "    {\"doc_type\": \"policy\", \"source\": \"policy.pdf\", \"section\": \"5.1\", \"page\": 20},\n",
        "    {\"doc_type\": \"policy\", \"source\": \"policy.pdf\", \"section\": \"overview\", \"page\": 1},\n",
        "    {\"doc_type\": \"policy\", \"source\": \"policy.pdf\", \"section\": \"4.3\", \"page\": 13},\n",
        "]\n",
        "\n",
        "existing = set(collection.get(ids=ids).get(\"ids\", []))\n",
        "to_add = [(d, i, m) for d, i, m in zip(docs, ids, metas) if i not in existing]\n",
        "\n",
        "if to_add:\n",
        "    collection.add(\n",
        "        documents=[d for d, _, _ in to_add],\n",
        "        ids=[i for _, i, _ in to_add],\n",
        "        metadatas=[m for _, _, m in to_add],\n",
        "    )\n",
        "    print(f\"Added {len(to_add)} policy items. Total:\", collection.count())\n",
        "else:\n",
        "    print(\"Policy demo items already exist. Total:\", collection.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "29369873",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adapter: Chroma -> ask_rag retriever schema (compatible with your Chroma)\n",
        "from typing import List, Dict\n",
        "Passage = Dict[str, object]\n",
        "\n",
        "def make_chroma_retriever(collection):\n",
        "    def _retriever(query: str, k: int = 30) -> List[Passage]:\n",
        "        # Remove \"ids\" from include (not supported in your version)\n",
        "        res = collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=k,\n",
        "            include=[\"documents\", \"metadatas\", \"distances\"]  # safe set\n",
        "        )\n",
        "        docs  = res.get(\"documents\", [[]])[0]\n",
        "        metas = res.get(\"metadatas\", [[]])[0]\n",
        "        out: List[Passage] = []\n",
        "        for i, text in enumerate(docs):\n",
        "            if not text:\n",
        "                continue\n",
        "            meta = dict(metas[i] or {}) if i < len(metas) else {}\n",
        "            # If you stored an id in metadata during ingestion it will be here; otherwise skip\n",
        "            out.append({\"text\": text, \"meta\": meta})\n",
        "        return out\n",
        "    return _retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "8d085034",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved: 5\n",
            "Sample: {'text': 'Overview: Our policy covers data retention, encryption, and access control.', 'meta': {'doc_type': 'policy', 'page': 1, 'section': 'overview', 'source': 'policy.pdf'}}\n"
          ]
        }
      ],
      "source": [
        "my_retriever = make_chroma_retriever(collection)\n",
        "\n",
        "test_hits = my_retriever(\"data retention\", k=5)\n",
        "print(\"Retrieved:\", len(test_hits))\n",
        "print(\"Sample:\", test_hits[0] if test_hits else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "913fefff",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ask_rag() got an unexpected keyword argument 'retriever'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res[\u001b[33m\"\u001b[39m\u001b[33mpassages\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][:\u001b[32m160\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m res.get(\u001b[33m\"\u001b[39m\u001b[33mpassages\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m k = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(collection, \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[32m12\u001b[39m)(), \u001b[32m12\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m out_no = \u001b[43mask_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat does clause 4.2 say about data retention?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmy_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_rerank\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mkeep_top_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m out_yes = ask_rag(\u001b[33m\"\u001b[39m\u001b[33mWhat does clause 4.2 say about data retention?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m                   retriever=my_retriever, k=k, enable_rerank=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m                   rerank_k=k, keep_top_n=\u001b[32m3\u001b[39m, generator=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTop‑1 without rerank:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, top1(out_no))\n",
            "\u001b[31mTypeError\u001b[39m: ask_rag() got an unexpected keyword argument 'retriever'"
          ]
        }
      ],
      "source": [
        "def top1(res): \n",
        "    return res[\"passages\"][0][\"text\"][:160] if res.get(\"passages\") else \"\"\n",
        "\n",
        "k = min(getattr(collection, \"count\", lambda: 12)(), 12)\n",
        "\n",
        "out_no = ask_rag(\"What does clause 4.2 say about data retention?\",\n",
        "                 retriever=my_retriever, k=k, enable_rerank=False,\n",
        "                 keep_top_n=3, generator=None)\n",
        "\n",
        "out_yes = ask_rag(\"What does clause 4.2 say about data retention?\",\n",
        "                  retriever=my_retriever, k=k, enable_rerank=True,\n",
        "                  rerank_k=k, keep_top_n=3, generator=None)\n",
        "\n",
        "print(\"Top‑1 without rerank:\\n\", top1(out_no))\n",
        "print(\"\\nTop‑1 with rerank:\\n\", top1(out_yes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928b5879",
      "metadata": {},
      "outputs": [],
      "source": [
        "def debug_rerank(query, retriever, k=12, rerank_k=12, keep_top_n=3):\n",
        "    pool = retriever(query, k=k)\n",
        "    texts = [p[\"text\"] for p in pool]\n",
        "    scores = bge_reranker(query, texts)  # None if package/model missing\n",
        "    print(\"pool:\", len(pool), \"scores_none:\", scores is None)\n",
        "    if scores:\n",
        "        ranked = sorted(zip(range(len(pool)), pool, scores), key=lambda x: x[2], reverse=True)\n",
        "        print(\"Top-5 by reranker:\")\n",
        "        for i, (idx, item, s) in enumerate(ranked[:5], 1):\n",
        "            txt = item['text'][:80].replace('\\n',' ')\n",
        "            print(f\"{i}. score={s:.3f}  text={txt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd139375",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pool: 12 scores_none: True\n"
          ]
        }
      ],
      "source": [
        "debug_rerank(\"What does clause 4.2 say about data retention?\", my_retriever, k=k, rerank_k=k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02e866e",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "34967d3a",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fd514b45",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d36dac9",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'collection' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _retriever\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Bind to the already-open 'collection' from the notebook\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m my_retriever = make_chroma_retriever(\u001b[43mcollection\u001b[49m)\n",
            "\u001b[31mNameError\u001b[39m: name 'collection' is not defined"
          ]
        }
      ],
      "source": [
        "# Adapter: Chroma -> ask_rag retriever schema\n",
        "from typing import List, Dict\n",
        "Passage = Dict[str, object]\n",
        "\n",
        "def make_chroma_retriever(collection):\n",
        "    def _retriever(query: str, k: int = 30) -> List[Passage]:\n",
        "        res = collection.query(query_texts=[query], n_results=k, include=[\"documents\",\"metadatas\",\"ids\"])\n",
        "        docs = res.get(\"documents\", [[]])[0]\n",
        "        metas = res.get(\"metadatas\", [[]])[0]\n",
        "        ids   = res.get(\"ids\", [[]])[0]\n",
        "        out: List[Passage] = []\n",
        "        for i, text in enumerate(docs):\n",
        "            if not text: \n",
        "                continue\n",
        "            meta = dict(metas[i] or {}) if i < len(metas) else {}\n",
        "            if i < len(ids): meta.setdefault(\"id\", ids[i])\n",
        "            out.append({\"text\": text, \"meta\": meta})\n",
        "        return out\n",
        "    return _retriever\n",
        "\n",
        "# Bind to the already-open 'collection' from the notebook\n",
        "my_retriever = make_chroma_retriever(collection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07531388",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal types to keep things clear\n",
        "from typing import List, Dict, Callable, Optional, Tuple\n",
        "\n",
        "# A passage is just text plus optional metadata (to keep citations)\n",
        "Passage = Dict[str, object]  # expects at least: {\"text\": str, ...}\n",
        "\n",
        "# ---- Optional local reranker (open-source) ----\n",
        "# Uses BGE Reranker if installed; otherwise returns None to skip rerank.\n",
        "# pip install -U FlagEmbedding   # (run once in your notebook)\n",
        "_bge_model_cache = {\"model\": None}\n",
        "\n",
        "def bge_reranker(query: str, passages: List[str]) -> Optional[List[float]]:\n",
        "    \"\"\"\n",
        "    Scores [query, passage] pairs using BGE Reranker if available.\n",
        "    Returns a list of floats (one score per passage) or None if not available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from FlagEmbedding import FlagReranker\n",
        "        if _bge_model_cache[\"model\"] is None:\n",
        "            # 'base' is lighter; you can also try 'BAAI/bge-reranker-large'\n",
        "            _bge_model_cache[\"model\"] = FlagReranker(\"BAAI/bge-reranker-base\", use_fp16=True)\n",
        "        model = _bge_model_cache[\"model\"]\n",
        "        pairs = [[query, p] for p in passages]\n",
        "        scores = model.compute_score(pairs)  # higher is better\n",
        "        # Ensure it's a plain list[float]\n",
        "        return [float(s) for s in (scores if isinstance(scores, list) else [scores])]\n",
        "    except Exception:\n",
        "        return None  # if package/model unavailable, fail safe\n",
        "\n",
        "# ---- Tiny helpers ----\n",
        "def assemble_context(passages: List[Passage], sep: str = \"\\n\\n---\\n\\n\") -> str:\n",
        "    \"\"\"\n",
        "    Joins top passages for the prompt. Keeps basic metadata if present.\n",
        "    \"\"\"\n",
        "    blocks = []\n",
        "    for p in passages:\n",
        "        t = str(p.get(\"text\", \"\"))\n",
        "        src = p.get(\"meta\") or p.get(\"source\") or p.get(\"id\")\n",
        "        if src:\n",
        "            blocks.append(f\"[Source: {src}]\\n{t}\")\n",
        "        else:\n",
        "            blocks.append(t)\n",
        "    return sep.join(blocks)\n",
        "\n",
        "# ---- Main function with rerank ----\n",
        "def ask_rag(\n",
        "    query: str,\n",
        "    retriever: Callable[[str, int], List[Passage]],  # your Chapter 4 retrieval function\n",
        "    k: int = 30,                 # initial retrieval pool\n",
        "    enable_rerank: bool = True,  # toggle rerank\n",
        "    rerank_k: int = 30,          # how many to score (<= k)\n",
        "    keep_top_n: int = 5,         # final top-N\n",
        "    reranker: Optional[Callable[[str, List[str]], Optional[List[float]]]] = bge_reranker,\n",
        "    generator: Optional[Callable[[str, str], str]] = None,  # optional LLM answerer\n",
        ") -> Dict[str, object]:\n",
        "    \"\"\"\n",
        "    Retrieve -> (optional) rerank -> assemble context -> (optional) generate answer.\n",
        "    Returns dict with 'context' and 'passages', plus 'answer' if generator provided.\n",
        "    \"\"\"\n",
        "    # 1) Retrieve candidates (vector or hybrid)—must return list of dicts with \"text\"\n",
        "    candidates = retriever(query, k=k) or []\n",
        "    # Normalize minimal fields\n",
        "    candidates = [c for c in candidates if isinstance(c, dict) and \"text\" in c]\n",
        "    if not candidates:\n",
        "        result = {\"answer\": \"\", \"context\": \"\", \"passages\": []}\n",
        "        return result if generator else {\"context\": \"\", \"passages\": []}\n",
        "\n",
        "    # 2) Optional rerank on a modest pool\n",
        "    top_passages = candidates[:keep_top_n]\n",
        "    if enable_rerank and len(candidates) > 1:\n",
        "        pool = candidates[:min(rerank_k, len(candidates))]\n",
        "        texts = [str(c[\"text\"]) for c in pool]\n",
        "        scores = (reranker(query, texts) if reranker else None)\n",
        "\n",
        "        if scores and len(scores) == len(pool):\n",
        "            # Sort by reranker score (descending)\n",
        "            ranked: List[Tuple[Passage, float]] = sorted(\n",
        "                zip(pool, scores),\n",
        "                key=lambda x: x[1],\n",
        "                reverse=True\n",
        "            )\n",
        "            top_passages = [c for c, _ in ranked[:keep_top_n]]\n",
        "        else:\n",
        "            # Fail safe: keep original order (no rerank)\n",
        "            top_passages = candidates[:keep_top_n]\n",
        "\n",
        "    # 3) Build context for prompting\n",
        "    context = assemble_context(top_passages)\n",
        "\n",
        "    # 4) Optionally generate an answer with your LLM of choice\n",
        "    if generator is not None:\n",
        "        try:\n",
        "            answer = generator(query, context)\n",
        "        except Exception:\n",
        "            answer = \"\"\n",
        "        return {\"answer\": answer, \"context\": context, \"passages\": top_passages}\n",
        "\n",
        "    # If no generator supplied, return context + passages so you can inspect\n",
        "    return {\"context\": context, \"passages\": top_passages}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d57e9d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example adapter for your existing Chapter 4 retrieval\n",
        "def my_retriever(query: str, k: int = 30) -> List[Passage]:\n",
        "    # Return a list of {\"text\": str, ...}. You can include {\"meta\": {...}} for citations.\n",
        "    # Replace this with your actual retrieval (vector or hybrid).\n",
        "    results = []  # <- fill from your DB/search call\n",
        "    # results should look like: [{\"text\": hit.text, \"meta\": {\"id\": hit.id, \"source\": hit.src}}, ...]\n",
        "    return results\n",
        "\n",
        "# Optional: your LLM answerer\n",
        "def my_generator(query: str, context: str) -> str:\n",
        "    # Replace with your OpenAI call or whichever model you use\n",
        "    # Return a plain string answer\n",
        "    return f\"(demo) I would answer '{query}' using this context:\\n{context[:300]}...\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8dfdeda",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            " \n",
            "\n",
            "Answer:\n",
            " \n"
          ]
        }
      ],
      "source": [
        "out = ask_rag(\n",
        "    query=\"What does clause 4.2 say about data retention?\",\n",
        "    retriever=my_retriever,\n",
        "    k=30,\n",
        "    enable_rerank=True,\n",
        "    rerank_k=30,\n",
        "    keep_top_n=5,\n",
        "    reranker=bge_reranker,  # uses BGE locally if installed; skip if not\n",
        "    generator=my_generator   # optional\n",
        ")\n",
        "\n",
        "print(\"Context:\\n\", out[\"context\"][:800])\n",
        "if \"answer\" in out:\n",
        "    print(\"\\nAnswer:\\n\", out[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba31cdfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "retrieved: 0\n",
            "sample item: None\n"
          ]
        }
      ],
      "source": [
        "q = \"What does clause 4.2 say about data retention?\"\n",
        "cands = my_retriever(q, k=30)\n",
        "print(\"retrieved:\", 0 if cands is None else len(cands))\n",
        "print(\"sample item:\", cands[0] if cands else None)\n",
        "\n",
        "# Must be list of dicts with at least a 'text' key\n",
        "if cands:\n",
        "    print(\"has text key:\", isinstance(cands[0], dict) and (\"text\" in cands[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a03285",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "74f758ce",
      "metadata": {},
      "source": [
        "## Putting It Together — Mini Demos\n",
        "\n",
        "This section adds four small, self-contained demos with simple trace output to illustrate key techniques:\n",
        "- Two-hop agent loop with citations\n",
        "- Hybrid + rerank ordering change (observe top-5)\n",
        "- Recency-sensitive ranking (time-decay blend)\n",
        "- PDF figure/caption question that returns the correct figure reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "c8328272",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mini Demo Utilities: simple JSONL trace logger\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "\n",
        "def log_trace(event: dict, logfile: str = 'chapter5_traces.jsonl') -> None:\n",
        "    ev = dict(event)\n",
        "    ev['ts'] = ev.get('ts') or time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())\n",
        "    p = Path(logfile)\n",
        "    with p.open('a', encoding='utf-8') as f:\n",
        "        print(json.dumps(ev, ensure_ascii=False), file=f)\n",
        "    print(f'Trace written: {p.resolve()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f10b9f9",
      "metadata": {},
      "source": [
        "In my experience, the best way to make retrieval demos teachable is to show what actually happened step by step. The `log_trace()` helper gives us a zero-dependency way to record those steps as JSON lines. Each time we call `log_trace({...})`, it appends one JSON object to `chapter5_traces.jsonl` (same folder as this notebook), adds a UTC timestamp `ts`, and returns quietly. This way, we can later inspect agent hops, retrieval variants, reranker decisions, and figure matches without cluttering the notebook output. It’s simple, robust, and works both locally and in Colab because it’s just file I/O.\n",
        "\n",
        "### Results Explanation\n",
        "- The function writes one line per event (JSONL). That makes it easy to stream, grep, or parse later.\n",
        "- Fields are whatever you pass in `event` plus an automatic `ts`.\n",
        "- You’ll see a brief confirmation printed with the absolute path to the trace file.\n",
        "\n",
        "### Context\n",
        "- Use this with the mini demos to capture internal steps:\n",
        "  - Agent loop: hop number, sub‑query, retrieved ids.\n",
        "  - Hybrid/rerank: which mode ran, top‑k preview, score info.\n",
        "  - Recency demo: parameters (alpha, half‑life) and resulting order.\n",
        "  - Figure/caption: matched figure id, page, and match score.\n",
        "- This aligns with the observability theme in Chapter 5: we don’t just return answers; we preserve the reasoning trail.\n",
        "\n",
        "### Next Steps\n",
        "- Call `log_trace({...})` at the key checkpoints in your demo code.\n",
        "- Keep event names consistent (for example, `agent_hop`, `agent_stop`, `hybrid_demo_run`, `figure_match`).\n",
        "- If the file grows too large while experimenting, delete `chapter5_traces.jsonl` and re-run cells to generate a fresh trace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8bc3b1e",
      "metadata": {},
      "source": [
        "### Mini Demo 1: Two-hop agent loop with citations\n",
        "\n",
        "A lightweight loop that Plans → Retrieves → Synthesizes → Decides. It logs each hop and returns citations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e0ed8cd",
      "metadata": {},
      "source": [
        "I’ve kept this agent loop intentionally small because the goal is to show the pattern without hiding behind complexity. We start with `DEMO_SNIPPETS` as a tiny in‑memory corpus, use `retrieve_snippets()` for quick lexical hits (token overlap), then `synthesize_answer()` builds a concise answer with clear citations. The `agent_loop()` runs a few hops: it plans a sub‑query, retrieves evidence, accumulates context, and decides whether to stop when we’ve got enough citations. Along the way, we record exactly what happened with `log_trace()` so you can inspect the reasoning trail later instead of guessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "468a6a44",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trace written: /Users/relhousieny/code/personal/books/data-strategy-book/27July2025/.public_repo/chapter_05/Jupyter_Notebooks/chapter5_traces.jsonl\n",
            "Trace written: /Users/relhousieny/code/personal/books/data-strategy-book/27July2025/.public_repo/chapter_05/Jupyter_Notebooks/chapter5_traces.jsonl\n",
            "Trace written: /Users/relhousieny/code/personal/books/data-strategy-book/27July2025/.public_repo/chapter_05/Jupyter_Notebooks/chapter5_traces.jsonl\n",
            "Based on the retrieved evidence, here is a concise answer.\n",
            "- Service X outage on 2024-07-12 impacted region us-west. [source=status_page, id=doc:A1, page=1]\n",
            "- Credential rotation policy updated in clause 4.2. [source=policy, id=doc:D4, page=7]\n",
            "Citations: [{'id': 'doc:A1', 'source': 'status_page', 'page': 1}, {'id': 'doc:D4', 'source': 'policy', 'page': 7}]\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# Simple in-memory snippets to keep the demo self-contained\n",
        "DEMO_SNIPPETS = [\n",
        "    {\"id\": \"doc:A1\", \"text\": \"Service X outage on 2024-07-12 impacted region us-west.\", \"source\": \"status_page\", \"page\": 1},\n",
        "    {\"id\": \"doc:B2\", \"text\": \"Mitigation steps included failover and cache flush.\", \"source\": \"postmortem\", \"page\": 2},\n",
        "    {\"id\": \"doc:C3\", \"text\": \"Root cause: stale credentials in deployment pipeline.\", \"source\": \"postmortem\", \"page\": 3},\n",
        "    {\"id\": \"doc:D4\", \"text\": \"Credential rotation policy updated in clause 4.2.\", \"source\": \"policy\", \"page\": 7},\n",
        "]\n",
        "\n",
        "# Tiny retrieval that ranks by token overlap\n",
        "def retrieve_snippets(query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
        "    q = set(t.lower() for t in query.split())\n",
        "    scored = []\n",
        "    for s in DEMO_SNIPPETS:\n",
        "        t = set(s['text'].lower().split())\n",
        "        score = len(q & t)\n",
        "        if score:\n",
        "            scored.append((score, s))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [s for _, s in scored[:k]]\n",
        "\n",
        "# Minimal synthesis with citations\n",
        "def synthesize_answer(question: str, snippets: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    if not snippets:\n",
        "        return {\"answer\": \"Insufficient evidence to answer.\", \"citations\": []}\n",
        "    lines = [f\"- {s['text']} [source={s['source']}, id={s['id']}, page={s.get('page','?')}]\" for s in snippets]\n",
        "    answer = \"Based on the retrieved evidence, here is a concise answer.\\n\" + \\\n",
        "             \"\\n\".join(lines)\n",
        "    citations = [{\"id\": s['id'], \"source\": s['source'], \"page\": s.get('page')} for s in snippets]\n",
        "    return {\"answer\": answer, \"citations\": citations}\n",
        "\n",
        "# Agent loop\n",
        "\n",
        "def agent_loop(query: str, max_hops: int = 3, min_citations: int = 2) -> Dict[str, Any]:\n",
        "    trace = {\"type\": \"agent_loop\", \"query\": query, \"hops\": []}\n",
        "    accumulated: List[Dict[str, Any]] = []\n",
        "    subq: Optional[str] = query\n",
        "\n",
        "    for hop in range(1, max_hops + 1):\n",
        "        plan = f\"Hop {hop}: retrieve evidence related to: '{subq}'\"\n",
        "        retrieved = retrieve_snippets(subq, k=3)\n",
        "        accumulated.extend([r for r in retrieved if r not in accumulated])\n",
        "\n",
        "        synth = synthesize_answer(query, accumulated)\n",
        "        hop_rec = {\n",
        "            \"hop\": hop,\n",
        "            \"plan\": plan,\n",
        "            \"sub_query\": subq,\n",
        "            \"retrieved_ids\": [r['id'] for r in retrieved],\n",
        "            \"total_citations\": len(synth['citations']),\n",
        "        }\n",
        "        trace['hops'].append(hop_rec)\n",
        "        log_trace({\"event\": \"agent_hop\", **hop_rec})\n",
        "\n",
        "        # Stop condition: enough citations or no new evidence\n",
        "        if synth['citations'] and len(synth['citations']) >= min_citations:\n",
        "            trace['stop_reason'] = \"enough_citations\"\n",
        "            log_trace({\"event\": \"agent_stop\", \"reason\": trace['stop_reason'], \"hop\": hop})\n",
        "            return {\"answer\": synth['answer'], \"citations\": synth['citations'], \"trace\": trace}\n",
        "\n",
        "        # Simple next-step refinement (toy): focus on policy if policy not seen\n",
        "        if not any(r['source'] == 'policy' for r in accumulated):\n",
        "            subq = \"policy credential rotation clause\"\n",
        "        else:\n",
        "            subq = None\n",
        "\n",
        "        if not subq:\n",
        "            trace['stop_reason'] = \"no_next_step\"\n",
        "            log_trace({\"event\": \"agent_stop\", \"reason\": trace['stop_reason'], \"hop\": hop})\n",
        "            return {\"answer\": synth['answer'], \"citations\": synth['citations'], \"trace\": trace}\n",
        "\n",
        "    trace['stop_reason'] = \"max_hops\"\n",
        "    log_trace({\"event\": \"agent_stop\", \"reason\": trace['stop_reason'], \"hop\": max_hops})\n",
        "    final = synthesize_answer(query, accumulated)\n",
        "    return {\"answer\": final['answer'], \"citations\": final['citations'], \"trace\": trace}\n",
        "\n",
        "# Quick run\n",
        "agent_result = agent_loop(\"Was the outage due to credentials? Provide sources.\")\n",
        "print(agent_result['answer'])\n",
        "print(\"Citations:\", agent_result['citations'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49bcfe7",
      "metadata": {},
      "source": [
        "### Results Explanation\n",
        "- You’ll see a final printed answer followed by “Citations:” containing `id`, `source`, and `page`.  \n",
        "- Each hop logs an event via `log_trace()` (e.g., `agent_hop`, `agent_stop`) with the sub‑query and retrieved IDs, which makes the process auditable.\n",
        "\n",
        "### Context\n",
        "- This is a minimal two‑hop agent pattern meant for teaching: plan → retrieve → synthesize → decide.  \n",
        "- The retrieval is intentionally simple (lexical overlap) so the focus stays on the loop and traceability, not on embeddings or DB setup.\n",
        "\n",
        "### Next Steps\n",
        "- Swap `retrieve_snippets()` for your real retriever (vector, hybrid, rerank) while keeping the same `agent_loop()` shape.  \n",
        "- Add small guardrails: hop budget, max context length, and a simple “no progress” detector.  \n",
        "- Enrich `log_trace()` events with `query_id`, collection names, and timing to feed observability later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb829444",
      "metadata": {},
      "source": [
        "### Mini Demo 2: Hybrid + Rerank (observe top-5)\n",
        "\n",
        "Re-run the hybrid demo and show how ordering changes. If the hybrid-enabled `ask_rag()` is present, we'll call it with flags and print the top-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "aca8cfbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector-only (top-5 preview):\n",
            "QUERY: 'Show me bug report #12345'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to enterprise_bug_demo\n",
            "\n",
            "STEP 1 (Vector-only): Searching for documents similar to: 'Show me bug report #12345'\n",
            "RESULTS: Found 2 documents:\n",
            "  1. ID: bug://12343 | Similarity: -0.500 | Preview: Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "  2. ID: bug://12347 | Similarity: -0.508 | Preview: Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 497 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "\n",
            "Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "Description: Authentication timeout issues reported by multiple users during login.\n",
            "Desktop login works but mobile shows various timeout behaviors.\n",
            "Reporter: alex.kim@company.com\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 835 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "\n",
            "Bug Report #12347\n",
            "Status: In Progress\n",
            "Component: Mobile App\n",
            "Description: Authentication timeout issues reported by multiple users during login.\n",
            "Desktop login works but mobile shows various timeout behaviors.\n",
            "Reporter: alex.kim@company.com\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Show me bug report #12345\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: I cannot find the information for bug report #12345 in the provided context.\n",
            "==================================================\n",
            "\n",
            "Hybrid enabled (top-5 preview):\n",
            "QUERY: 'Show me bug report #12345'\n",
            "==================================================\n",
            "STATUS: Reusing existing ChromaDB client\n",
            "COLLECTION: Connected to enterprise_bug_demo\n",
            "\n",
            "STEP 1 (Hybrid): Vector candidate pool + keyword re-scoring\n",
            "RESULTS: Re-ranked top 2 by hybrid score\n",
            "  1. ID: bug://12345 | v=0.933 kw=1.000 mix=0.977 | Preview: Bug Report #12345\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "  2. ID: bug://12343 | v=1.000 kw=0.614 mix=0.749 | Preview: Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication...\n",
            "\n",
            "CONTEXT: Combined 2 documents into context\n",
            "CONTEXT LENGTH: 657 characters\n",
            "\n",
            "FULL CONTEXT:\n",
            "----------------------------------------\n",
            "Bug Report #12345\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.\n",
            "Steps to reproduce: 1) Open mobile app 2) Click OAuth login 3) Wait 35 seconds\n",
            "Expected: Login completes successfully\n",
            "Actual: Timeout error displayed\n",
            "Reporter: sarah.chen@company.com\n",
            "Assigned: mike.torres@company.com\n",
            "Created: 2024-03-15\n",
            "\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "----------------------------------------\n",
            "\n",
            "STEP 2: Constructing prompt with context and query\n",
            "PROMPT LENGTH: 995 characters\n",
            "\n",
            "FULL PROMPT BEING SENT TO LLM:\n",
            "============================================================\n",
            "\n",
            "    You are an expert assistant. Use the following retrieved context to answer the user's question.\n",
            "    If the answer is not in the context, state that you cannot find the information.\n",
            "    Do not use any other information.\n",
            "\n",
            "    <context>\n",
            "    Bug Report #12345\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.\n",
            "Steps to reproduce: 1) Open mobile app 2) Click OAuth login 3) Wait 35 seconds\n",
            "Expected: Login completes successfully\n",
            "Actual: Timeout error displayed\n",
            "Reporter: sarah.chen@company.com\n",
            "Assigned: mike.torres@company.com\n",
            "Created: 2024-03-15\n",
            "\n",
            "Bug Report #12343\n",
            "Status: Critical\n",
            "Component: Authentication Service\n",
            "Description: Mobile OAuth login fails with timeout after extended periods of inactivity.\n",
            "Users report authentication issues specifically on mobile devices.\n",
            "Reporter: emma.davis@company.com\n",
            "    </context>\n",
            "\n",
            "    <question>\n",
            "    Show me bug report #12345\n",
            "    </question>\n",
            "\n",
            "    Answer:\n",
            "    \n",
            "============================================================\n",
            "\n",
            "STEP 3: Sending prompt to LLM\n",
            "\n",
            "LLM RESPONSE: Bug Report #12345  \n",
            "Status: Critical  \n",
            "Component: Authentication Service  \n",
            "Description: Login timeout occurs after 30 seconds on mobile devices when using OAuth2 flow.  \n",
            "Steps to reproduce:  \n",
            "1) Open mobile app  \n",
            "2) Click OAuth login  \n",
            "3) Wait 35 seconds  \n",
            "Expected: Login completes successfully  \n",
            "Actual: Timeout error displayed  \n",
            "Reporter: sarah.chen@company.com  \n",
            "Assigned: mike.torres@company.com  \n",
            "Created: 2024-03-15\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Attempt to call previously defined hybrid-enabled ask_rag\n",
        "try:\n",
        "    QUERY = \"Show me bug report #12345\"\n",
        "    COLLECTION = \"enterprise_bug_demo\"\n",
        "\n",
        "    print(\"Vector-only (top-5 preview):\")\n",
        "    _ = ask_rag(query=QUERY, collection_name=COLLECTION, debug=True, enable_hybrid=False, n_initial=20)\n",
        "\n",
        "    print(\"\\nHybrid enabled (top-5 preview):\")\n",
        "    _ = ask_rag(query=QUERY, collection_name=COLLECTION, debug=True, enable_hybrid=True, hybrid_weight=0.35, n_initial=30)\n",
        "except Exception as e:\n",
        "    print(\"Hybrid demo skipped:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f1fe6e0",
      "metadata": {},
      "source": [
        "### Mini Demo 3: Recency-sensitive ranking\n",
        "\n",
        "Blend semantic score with a time-decay factor to prioritize fresher content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "c5a1c757",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline (semantic only):\n",
            "  news3 | sem=0.85 | age=45d\n",
            "  news1 | sem=0.82 | age=90d\n",
            "  news2 | sem=0.80 | age=2d\n",
            "  news4 | sem=0.78 | age=1d\n",
            "Recency-blended:\n",
            "  news1 | sem=0.82 | age=90d\n",
            "  news2 | sem=0.80 | age=2d\n",
            "  news3 | sem=0.85 | age=45d\n",
            "  news4 | sem=0.78 | age=1d\n",
            "Trace written: /Users/relhousieny/code/personal/books/data-strategy-book/27July2025/.public_repo/chapter_05/Jupyter_Notebooks/chapter5_traces.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gp/yfkh9dk97kxcvkzqyxwrjb080000gp/T/ipykernel_5186/4004324010.py:17: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.utcnow()\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Simple utilities\n",
        "\n",
        "def time_decay(age_days: float, half_life_days: float = 30.0) -> float:\n",
        "    # Exponential decay: value halves every half_life_days\n",
        "    import math\n",
        "    return math.pow(0.5, age_days / max(1e-9, half_life_days))\n",
        "\n",
        "\n",
        "def recency_blend(sem_score: float, ts: datetime, now: datetime, half_life_days: float = 30.0, alpha: float = 0.7) -> float:\n",
        "    age_days = (now - ts).days + (now - ts).seconds / 86400.0\n",
        "    rec = time_decay(age_days, half_life_days)\n",
        "    return alpha * sem_score + (1 - alpha) * rec\n",
        "\n",
        "# In-memory items: (title, sem_score, timestamp)\n",
        "now = datetime.utcnow()\n",
        "items = [\n",
        "    {\"id\": \"news1\", \"title\": \"Policy updated\", \"sem\": 0.82, \"ts\": now - timedelta(days=90)},\n",
        "    {\"id\": \"news2\", \"title\": \"Hotfix released\", \"sem\": 0.80, \"ts\": now - timedelta(days=2)},\n",
        "    {\"id\": \"news3\", \"title\": \"Guidance note\", \"sem\": 0.85, \"ts\": now - timedelta(days=45)},\n",
        "    {\"id\": \"news4\", \"title\": \"FAQ refreshed\", \"sem\": 0.78, \"ts\": now - timedelta(days=1)},\n",
        "]\n",
        "\n",
        "print(\"Baseline (semantic only):\")\n",
        "for r in sorted(items, key=lambda x: x['sem'], reverse=True)[:5]:\n",
        "    print(f\"  {r['id']} | sem={r['sem']:.2f} | age={(now-r['ts']).days}d\")\n",
        "\n",
        "print(\"Recency-blended:\")\n",
        "for r in sorted(items, key=lambda x: recency_blend(r['sem'], r['ts'], now), reverse=True)[:5]:\n",
        "    print(f\"  {r['id']} | sem={r['sem']:.2f} | age={(now-r['ts']).days}d\")\n",
        "\n",
        "log_trace({\"event\": \"recency_demo_ran\", \"count\": len(items)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf959209",
      "metadata": {},
      "source": [
        "### Mini Demo 4: PDF figure/caption question → figure reference\n",
        "\n",
        "Pragmatic approach without extra PDF dependencies: we use a small figure index (caption + page) and match by keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "0c9bcbd5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trace written: /Users/relhousieny/code/personal/books/data-strategy-book/27July2025/.public_repo/chapter_05/Jupyter_Notebooks/chapter5_traces.jsonl\n",
            "Best match: Fig 5.6 on page 21.\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def answer_figure_query(query: str, index: List[Dict]) -> Dict:\n",
        "    q = set(query.lower().split())\n",
        "    best = None\n",
        "    best_score = -1\n",
        "    for rec in index:\n",
        "        caption_tokens = set(rec['caption'].lower().split())\n",
        "        text_tokens = set(rec.get('text','').lower().split())\n",
        "        score = len(q & (caption_tokens | text_tokens))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best = rec\n",
        "    if not best:\n",
        "        return {\"answer\": \"No matching figure found.\", \"figure\": None}\n",
        "    log_trace({\"event\": \"figure_match\", \"figure_id\": best['figure_id'], \"page\": best['page'], \"score\": best_score})\n",
        "    return {\n",
        "        \"answer\": f\"Best match: {best['figure_id']} on page {best['page']}.\",\n",
        "        \"figure\": {\"id\": best['figure_id'], \"page\": best['page'], \"caption\": best['caption']}\n",
        "    }\n",
        "\n",
        "FIGURE_INDEX = [\n",
        "    {\"figure_id\": \"Fig 5.3\", \"page\": 12, \"caption\": \"Search step inside retrieval blending semantic vectors with lexical BM25\", \"text\": \"hybrid retrieval pipeline search step\"},\n",
        "    {\"figure_id\": \"Fig 5.6\", \"page\": 21, \"caption\": \"Reranker reorders candidates using cross-encoder scores\", \"text\": \"semantic reranking top passages\"},\n",
        "    {\"figure_id\": \"Fig 5.7\", \"page\": 25, \"caption\": \"Figure/caption-aware retrieval path for document questions\", \"text\": \"caption extraction flow\"},\n",
        "]\n",
        "\n",
        "ans = answer_figure_query(\"Which figure shows the reranker improving ordering?\", FIGURE_INDEX)\n",
        "print(ans['answer'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Data Strategy Book)",
      "language": "python",
      "name": "data-strategy-book"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
